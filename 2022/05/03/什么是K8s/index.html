<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoon286.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="什么是K8sk8s解决的问题 让我们回顾一下为什么 Kubernetes 如此有用。">
<meta property="og:type" content="article">
<meta property="og:title" content="Kubernetes学习">
<meta property="og:url" content="https://yoon286.github.io/2022/05/03/%E4%BB%80%E4%B9%88%E6%98%AFK8s/index.html">
<meta property="og:site_name" content="yoon">
<meta property="og:description" content="什么是K8sk8s解决的问题 让我们回顾一下为什么 Kubernetes 如此有用。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/yoon286/Pic@main/img/202205031835022.png">
<meta property="article:published_time" content="2022-05-03T15:27:31.000Z">
<meta property="article:modified_time" content="2022-05-04T07:08:34.088Z">
<meta property="article:author" content="尹 丹">
<meta property="article:tag" content="云原生">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/yoon286/Pic@main/img/202205031835022.png">

<link rel="canonical" href="https://yoon286.github.io/2022/05/03/%E4%BB%80%E4%B9%88%E6%98%AFK8s/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Kubernetes学习 | yoon</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">yoon</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">风物长宜放眼量</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-首页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-时间轴">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>时间轴</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yoon286.github.io/2022/05/03/%E4%BB%80%E4%B9%88%E6%98%AFK8s/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="尹 丹">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yoon">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Kubernetes学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-05-03 23:27:31" itemprop="dateCreated datePublished" datetime="2022-05-03T23:27:31+08:00">2022-05-03</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  >
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E4%B8%AD%E9%97%B4%E4%BB%B6/" itemprop="url" rel="index"><span itemprop="name">中间件</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="什么是K8s"><a href="#什么是K8s" class="headerlink" title="什么是K8s"></a>什么是K8s</h1><h2 id="k8s解决的问题"><a href="#k8s解决的问题" class="headerlink" title="k8s解决的问题"></a>k8s解决的问题</h2><p><img src="https://cdn.jsdelivr.net/gh/yoon286/Pic@main/img/202205031835022.png" alt="image-20220503183514807"></p>
<p>让我们回顾一下为什么 Kubernetes 如此有用。</p>
<span id="more"></span>

<p><strong>传统部署时代：</strong></p>
<p>早期，各个组织机构在物理服务器上运行应用程序。无法为物理服务器中的应用程序定义资源边界，这会导致资源分配问题。</p>
<p> 例如，如果在物理服务器上运行多个应用程序，则可能会出现一个应用程序占用大部分资源的情况， 结果可能导致其他应用程序的性能下降。</p>
<p><strong>虚拟化部署时代：</strong></p>
<p>作为解决方案，引入了虚拟化。虚拟化技术允许你在单个物理服务器的 CPU 上运行多个虚拟机（VM）。</p>
<p><strong>容器部署时代：</strong></p>
<p>容器类似于 VM，但是它们具有被放宽的隔离属性，可以在应用程序之间共享操作系统（OS）。 因此，容器被认为是轻量级的。容器与 VM 类似，具有自己的文件系统、CPU、内存、进程空间等。</p>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><ul>
<li>敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。</li>
<li>持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性），支持可靠且频繁的 容器镜像构建和部署。</li>
<li>松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。z</li>
</ul>
<h2 id="K8s的特点"><a href="#K8s的特点" class="headerlink" title="K8s的特点"></a>K8s的特点</h2><ul>
<li><p><strong>服务发现和负载均衡</strong></p>
<p>Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。</p>
</li>
<li><p><strong>存储编排</strong></p>
<p>Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。</p>
</li>
<li><p><strong>自动部署和回滚</strong></p>
<p>你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态 更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。</p>
</li>
<li><p><strong>自动完成装箱计算</strong></p>
<p>Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。 当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。</p>
</li>
<li><p><strong>自我修复</strong></p>
<p>Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的 运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。</p>
</li>
<li><p><strong>密钥与配置管理</strong></p>
<p>Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。</p>
</li>
</ul>
<h3 id="Main-Components"><a href="#Main-Components" class="headerlink" title="Main Components"></a>Main Components</h3><p>左侧是一个官方提供的名为 <code>kubectl</code> 的 CLI （Command Line Interface）工具，用于使用 K8S 开放的 API 来管理集群和操作对象等。</p>
<p>Master 是一种角色（Role），表示在这个 Node 上包含着管理集群的一些必要组件。</p>
<p>Node 是工作节点 (worker Node) </p>
<p>K8s 系统是一套专注容器应用管理的集群系统，它的组件一般按功能分别部署在主控节点（master node）和计算节点(agent node)。对于主控节点，主要包含有 etcd 集群，controller manager 组件，scheduler 组件，api-server 组件。对于计算节点，主要包含 Pod、Service、ConfigMap、Volume、kubelet-proxy 组件。</p>
<p>容器的引入让原来主机仅可以部署 3-4 个进程的系统，现在可以充分利用容器进程隔离的技术在主机上部署 20-40 个进程系统，并且各自还不受影响。这就是容器应用的最大好处。</p>
<p>Kubernetes 是用于自动部署，扩展和管理容器化应用程序的开源系统，一般被 DevOps 团队用来解决在 CI/CD（也就是持续集成、持续发布）场景下遇到的工具链没法统一，构建过程没法标准化等痛点。</p>
<h2 id="文稿："><a href="#文稿：" class="headerlink" title="文稿："></a>文稿：</h2><p>2022年5月4日 下午 2:23|3小时36分钟54秒</p>
<p>说话人 1 02:22<br>So, in this part I’m going to explain what Kubernetes is we’re going to start off with the definition to see what official definition is and what it does, then we’re going to look at the problem solution case study of Kobe’s.</p>
<p>说话人 1 02:43<br> Kubernetes  is an open source container orchestration framework Which was originally developed by Google so on the foundation it manages containers be docker containers or from some other technology?Which basically means that commodities helps you manage applications that are made up of hundreds or maybe thousands of containers?And it helps you manage them in different environments.Like physical machines virtual machines or cloud environments or even hybrid deployment environment.So what problems does Kubernetes solve an what are the tasks of the container orchestration tool actually?So to go through this kernel logically the rise of microservices cause increased usage of container technologies because the containers actually offered the perfect host for.Small independent applications like Microservices and the rise of containers in the microservice technology actually resulted in applications that they’re now comprised of hundreds or sometimes maybe.Even thousands of containers now managing those loads of containers across multiple environments using scripts and self made tools can be really complex and sometimes even impossible.</p>
<p>说话人 1 04:08<br>So that’s specific scenario actually caused the need for having container orchestration technologies, So what those orchestration tools like Kubernetes do is actually guarantee following features.One is high availability in simple words high availability means that the application has no down time so it’s always accessible by the users a second one is scalability.Which means that application has a high performance it loads fast and the users have a very?High response rates from the application and the third one is disaster recovery, which basically means that if an infrastructure has some problems like data is lossed or the servers explode or something bad happens with the service center.The infrastructure has to have some kind of mechanism to pick up the data and to restore it to the latest state so that application doesn’t actually lose any data and the containerized application can run from the latest state.After the recovery and all of these are functionalities that container orchestration technologies like Kubernetes offer.</p>
<p>说话人 1 05:23<br>So, in this video I want to give you an overview of the most basic fundamental components of Kubernetes.But just enough to actually get you started using Kubernetes in practice, either as a dev OPS engineer.Or, a software developer now communities has tons of components, but most of the time you’re going to be working with just a handful of them.So I’m going to build a case of a simple JavaScript application with a simple database.And I’m going to show you step by step?How each component of Kubernetes actually helps you to deploy your application.And what is the role of each of those components.</p>
<p>说话人 1 06:07<br>So let’s start with the basic setup of a worker node or in Kubernetes terms on node.Which is a simple server a physical or virtual machine and the basic components or the smallest unit of Kubernetes is a pod So what part is is basically an abstraction over a container?So if you’re familiar with Docker containers or container images.So basically what part does is it creates this running environment or a layer on top of the container and the reason is because Kubernetes wants to.Abstract way the container runtime or container technologies so that you can replace them.If you want, too, and also because you don’t have to directly work with Docker or whatever.Container technology use in a Kubernetes so you only interact with carbonated layer.So we have an application part which is our own application and that will maybe use a database pod with its?Own container and this is also an important concept here.Part is usually meant to run.One application container inside of it, you can run multiple containers inside one.But usually it’s only the case.If you have one main application container in the helper container or some side service that has to run inside of that pod and uses it.This is nothing special.We just have one server and 2 containers running on it.With the abstraction layer on top of it.</p>
<p>说话人 1 07:38<br>So now let’s see how they communicate with each other in carbonated world.Securities offers out of the box of virtual network, which means that each pod gets its own IP address.Not the container the pod gets the IP address in each pod can communicate with each other using that IP address, which is an internal IP address.Obviously it’s not the public one so my application.Container can communicate with database using the IP address.</p>
<p>说话人 1 08:07<br>However, pod components in communities also an important concept are ephemeral, which means that they can die very easily.And when that happens for example, if I lose a database container because the container crashed because the application crashed inside or because the nodes.The server that I’m running them on.Raynard resources the pod will die in a new one will get created in its place.And when that happens, it will get assigned a new IP address, which obviously is inconvenient if you are.Communicating with the database using the IP address because now you have to adjust it every time pod restarts.And because of that another component of Kubernetes called service is used.So service is basically a static IP address or permanent IP address that can be attached.So to say to each pot.So my app will have its own service and database part will have its own service.And the good thing here is that the life cycles of service and the pod are not connected so even if the pod dies.The service and its IP address will stay so you don’t have to.</p>
<p>说话人 1 09:25<br>Change that endpoint anymore, so now obviously you would want your application to be accessible through a browser right and for this you would have to create an external service so external services.A service that opens the communication from external sources, but obviously you wouldn’t want your database to be open to the public requests and for that you would create something called an internal service.So this is a type of service that you specify when creating one.However, if you notice.The URL of the external service is not very practical so basically what you have.Is an HTTP protocol with a node IP address so off the node not the service and the port number of the service, which is good for test purposes.If you want to test something very fast.But not for the end product, so usually you would want your URL to look like this, if you want to talk to your application with a secure protocol and a domain name and for that there is another component of Kubernetes called?Increase so instead of service, the request goes first to Ingress and it does the forwarding then to the service so now we saw some of the very basic components of Kubernetes as you see this is a very simple setup.We just have one server.In a couple of containers running and some services nothing really special.Kubernetes advantages or the actual cool features really come forward.But we’re going to get there step by step, so let’s continue.So as we said parts communicate with each other using a service so my application will have a database endpoint.Let’s say called Mongo.Db service that it uses to communicate with the database.But where do you configure usually this database URL or endpoints usually you would do it in application properties file or as some kind of external environmental variable, but usually it’s inside of the built.Image of the application so for example, if the endpoints of the service or service name in this case changed to Mongo.Db you would have to adjust that URL in the application, so usually you would have to.Rebuild the application with the new version and you have to push it to the repository and now you’ll have to pull that gnu image in your pod and restart the whole thing so a little bit tedious for.A small change like database URL so for that purpose.</p>
<p>说话人 1 12:11<br>Kubernetes has a component called config map.So what it does is it’s basically your external configuration to your application so config map.Would usually contain configuration data like URLs of database or some other services.That use and in Kubernetes.You just connect it to the pod, so that part actually gets the data that config map.Contains and now if you change the name of the service.The endpoint of the service, you just adjust the config map and that’s it.You have to build a new image and have to go through this whole cycle now part.</p>
<p>说话人 1 12:48<br>Of the external configuration can also be database username and password right, which may also change in the application deployment process, but putting a password or other credentials in a config map.In a plain text format would be insecure, even though it’s an external configuration so for this purpose.</p>
<p>说话人 1 13:10<br>Kubernetes has another component called secret, so secret is just like config map.But the difference is that it’s used to store secret data credentials.For example, an it’s stored, not in a plain text format.Of course, but in base 64 encoded format so secret would contain things like.Credentials and of course I mean, database user.You could also put in config map.But what’s important is the passwords certificates.Things that you don’t want other people to have access to would go in the secret.And just like config map you just connect it to your part so that part can actually see those data and read from the secret.You can actually use the data from config map or secret inside of your application pod.</p>
<p>说话人 1 13:58<br>Using for example, environmental variables or even as a properties file so now to review.We’ve actually looked at almost all mostly used communities basic components.We’ve looked at the pod.We’ve seen how services are used?What is inggris component useful for and we’ve also seen external configuration using config map and secrets.</p>
<p>说话人 1 14:27<br>So now let’s see another very important concept generally which is data storage and how it works in Kubernetes.So we have this database pod that our application uses an it has some data.We generate some data with this setup that you see.</p>
<p>说话人 1 14:42<br>Now if the database container or the pod gets restarted.The data would be gone and that’s problematic, and inconvenient, obviously because.Do you want your database data or log data to be persisted reliably long term?And the way you can do it in communities is using another component of Kubernetes called volumes.And how it works is that it basically attaches a physical storage on a hard drive to your pot and that storage could be either on a local machine, meaning on the same server node where the pod is running.Or it could be on the remote storage, meaning outside of the cabinets cluster.It could be a cloud storage or could be your own premise storage, which is not part of the Kubernetes cluster so you just have an external reference on it.So now when the database pod, or container gets restarted all the data will be there persisted.It’s important to understand the distinction between the Kubernetes cluster and all of its components and the storage.Regardless of whether it’s a local or remote storage think of a storage as an external hard drive plugged in into the carbonated cluster because the point is communities clustered explicitly doesn’t manage.Any data persistence, which means that you as a quantities user or an administrator are responsible for backing up the data replicating and managing, it and making sure that it’s kept on a proper hardware.Etc because it’s not taking care of Kubernetes.</p>
<p>说话人 1 16:24<br>So now let’s see everything is running perfectly and a user can access our application through a browser.And with this set up what happens if my application pod dies right crushes or I have to restart the pod because I built knew container image.Basically, I would have a downtime where a user can reach my application, which is obviously a very bad thing if it happens.In production and this is exactly the advantage of distributed systems and containers.So instead of relying on just one application pod.In one database pod, etc.We are replicating everything on multiple servers, so we would have another node.Where a replica or clone of our application would run?Which will also be connected to the service so remember previously we said the services like and persistent static IP address.With the DNS name so that you don’t have to constantly adjust the endpoint when pod dies, but service is also a load balancer, which means that the service will actually catch the request and forward it to whichever party is least busy.So it has both of these functionalities, but in order to create the second replica of my application pod.You wouldn’t create a second pod, but instead you will define a blueprint for in my application pod.And specify how many replicas of that pod.You would like to run and that component or that blueprint is called deployment, which is another component of communities and in practice, you would not be working with pulse or you will not be creating.</p>
<p>说话人 1 18:11<br>Parts you would be creating deployements because there, you can specify how many replicas and you can also scale up or scale down.Number of replicas of parts that you need so with pod.We said that part is a layer of abstraction on top of containers and employment is another abstraction on top of pots, which makes it more convenient to interact with the parts.Replicate them and do some other configuration so in practice, you would mostly work with deployments and not with pots.So now if one of the replicas of your application pod would die the service will.Forward the requests to another one.So, your application would still be iaccessible for the user.</p>
<p>说话人 1 18:55<br>So now you’re probably wondering what about the database pod because if the database pod died your application also wouldn’t be.Accessible so we need a database replicas.Well, however, we can’t replicate database using a deployment and the reason for that is because database has state.Which is its data meaning that if we have clones are replicas of the database they would all need to access the same shared data storage and there, you would need some kind of mechanism.Some energies, which parts are currently writing to that storage or which parts are reading from the storage in order to avoid data inconsistencies and that mechanism.In addition to replicating feature is offered by another Kubernetes component called stateful set so this component is meant specifically for applications like databases.So my SQL, Mongo DB Elasticsearch or any other stateful applications or databases should be created using stateful sets and not deployments.It’s a very important distinction.And Stateful said just like Deployement would take care of replicating the pots and scaling them up or scaling them down, but making sure that database reads and writes are synchronized so that no database inconsistencies.Are offered?</p>
<p>说话人 1 20:28<br>However, I must mention here that deploying database applications using stateful sets in Kubernetes cluster can be somewhat tedious so it’s definitely more difficult than.Working with deployements where you don’t have all these challenges that’s why it’s also a common practice to host database applications outside of the Cabinet is cluster and just have the deployments or stateless applications.That replicate and scale with no problem inside of the Cabinet is cluster and communicate with external database.</p>
<p>说话人 1 21:04<br>So now that we have 2 replicas of my application pod and 2 replicas of the database and there, both.Balanced our setup is more robust, which means that now, even if node one.The whole node server was actually rebooted or crashed and nothing could run on it, we would still have a second note.With application and database pods running on it, and the application would still be accessible by the user.Until this 2 replicas get recreated so you can avoid downtime so to summarize.</p>
<p>说话人 1 21:39<br>We have looked at the most used combinators components.We started with the pods and services in order to communicate between the parts and the Ingress Component.Which is used to route traffic into the cluster.We’ve also looked at external configuration using config Maps and secrets and data persistence using?Volumes and finally we’ve looked at pod blueprints with replicating mechanisms like deployments and stateful sets where stateful set is used specifically for stateful applications like?Databases and yes, there are a lot more components that Kubernetes offers but these are really the core.The basic ones just using this core components you can actually build pretty powerful.</p>
<p>说话人 1 22:26<br>Kubernetes clusters.In this video we’re gonna talk about basic architecture of Kubernetes.So we’re going to look at 2 types of notes that Kubernetes operates on one is master in another one is slave.And we’re going to see what is the difference between those in which role each one of them has inside of the cluster and we’re going to go through the basic concepts of how Kubernetes does what it does and how the cluster.Is self managed and self healing an automated and how you as a operator of the companies cluster should end up having much less manual effort.And we’re going to start with this basic setup of one node with 2 application parts running on it, so one of the main components of carbonates architecture are its worker servers or notes.Each node will have multiple application pods with containers running on that node and the way communities does it is using 3 processes that must be installed on every node?That are used to schedule an manage those pots, so notes are the cluster servers that actually do the work.That’s why sometimes also called worker.</p>
<p>说话人 1 23:49<br>Notes so the first process that needs to run on every node is the container runtime.In my example, I have docker, but it could be some other technology as well so because application pods have containers running inside a container runtime needs to be installed on every note, but the process that actually schedules.Those those pods in the containers, then underneath is kubelet, which is a process of Kubernetes itself, unlike container runtime that has interface with both.Container runtime end the machine the note itself because at the end of the day.Kubelet is responsible for taking that configuration and actually running a pod or starting a pod with a container inside.And then assigning resources from that node to the container like CPU ram and storage resources.So usually Kubernetes cluster is made up of multiple nodes which also must have?Container runtime and Kubelet Service is installed, and you can have hundreds of those worker notes, which will run.Other pots and containers in replicas of the existing parts like my app and database parts in this example.And the way that communication between them works is using services which is sort of a load balancer.That basically catches the request directed to the pod or the application like database for example.And then forwards it to the respective pot, and the third process that is responsible for forwarding requests from services to pods is actually Q Proxy that also must be installed.On every note in Q Proxy has actually intelligent forwarding logic inside, that makes sure that.</p>
<p>说话人 1 25:40<br>Communication also works in a performance way with low overhead.For example, if an application Myapp replica is making a request database instead of service just randomly.Forwarding the requests to any replica it will actually forward it to the replica that is running on the same node as the pod that initiated the request, thus this way, avoiding the network overhead.Of sending the request to another machine, so to summarize 2 Kubernetes processes kubelet and Q Proxy must be installed on every Kubernetes worker node.Along with an independent container runtime in order for Kubernetes cluster to function properly, but now the question is how do you interact with this cluster or do you decide on which nodes a new application pod?Or database part should be scheduled or if replica pod dies.What process actually monitor is it an then reschedules it or restarts.It again or when we add another server?How does it join the cluster?To become another nose and get parts and other components created on it, and the answer is all these managing processes are done by master notes.So must have servers or must notes have completely different processes running inside and these are 4 processes that run on every masternode that control the cluster state and the worker nodes as well.So the first service is API server, so when you as a user want to deploy a new application and accommodate this cluster.You interact with the API server using some client it could be.Why like Kubernetes dashboard could be command line tool like kubelet or a Kubernetes API O API server is like a cluster gateway, which gets the initial requests of.Any updates into the cluster or even the queries from the cluster and it also acts as a gatekeeper for authentication to make sure that only authenticated and authorized requests get.Through to the cluster that means whenever you want to schedule new pots, diploid new applications create new service or any other components.</p>
<p>说话人 1 28:03<br>You have to talk to the API server on the master node and the API server.Then validate your request and if everything is fine.Then it will forward your request to other processes in order to schedule the pod or create this component that you requested.And also, if you want to query.The status of your deployment or the cluster health etc.You make a request to the API server and it gives you the response, which is good for security because you just have one entry point into the cluster.Another must process is a scheduler so as I mentioned if you.Send an API server request to schedule a new pod.Api server after its validates your request will actually hand it over to the scheduler in order to start that application.Hold on one of the worker nodes and of course, instead of Justice randomly, assigning it to any node schedule has this whole intelligent way of deciding on which specific worker node.The next pod will be scheduled or next component, will be scheduled so first it will look at your request and see how much resources.The application that you want.Schedule will need how much CPU how much ram and then it’s going to look at and it’s going to go through the worker notes and see the available resources on each one of them and if it sees that.Onenote is.The least busy or has the most resources available.It will schedule the new part on that note an important point here is that scheduler just decides on which nodes and new pod will be scheduled that process that.Actually does the scheduling that actually starts that pod with a container is the kubelet so it gets the request from the scheduler and executes that request on that note the next component.</p>
<p>说话人 1 30:05<br>Is controller manager which is another crucial component because?What happens when pods die on any note there must be a way to detect that nodes died and then reschedule.Those parts as soon as possible So what controller manager does is detect state changes like crashing of pods for example, so when pots die controller, manager detects that.And tries to recover.The cluster state as soon as possible on for that it makes a request for the scheduler to reschedule those dead parts in the same cycle happens here where the scheduler decides based on the resource calculation.Which worker notes should restart those pots again and makes requests to the corresponding cubelets on those worker nodes to actually restart the pots and finally the last master.</p>
<p>说话人 1 31:02<br>Process is at city, which is a key value store of a cluster state.You can think of it as a cluster brain.Actually, which means that every change in the cluster for example, when a?New pod get scheduled when a pod dies all of these changes get saved or updated into this key value store of Etsitty and the reason why it’s a D story is a cluster brain is because.</p>
<p>说话人 1 31:29<br>All of these mechanism with scheduler controller, manager etc works because of its data.So for example, how does scheduler know what resources are available or on each worker node or how does controller manager know that a cluster state changed in some way for example.Pods died?Or that Kubelet restarted new pots upon the request of a scheduler or when you make a query request to API server about the cluster health or for example, your application deployment state.Where does API server get all this state information from so all of this information is stored in SD cluster?What is not stored in the HDD key value store is the actual application data for example, if you have a?Database application running inside of the cluster that data will be stored somewhere else not in that city.This is just a cluster state information, which is used for master processes to communicate with them work processes.And vice versa, so now you probably already see that master processes are absolutely crucial for the cluster operation, especially the it’s a DS store, which contains some data must be reliable E stored.Or replicated so in practice communities cluster is usually made up of multiple Masters, where each masternode runs its master processes where of course, the API.Server is load balanced and the Exedy store forms at distributed storage across all the Masternodes.</p>
<p>说话人 1 33:09<br>So now that we saw what processes run on worker nodes an master notes.Let’s actually have a look at and really stick example of a cluster setup so in a very small cluster you’d probably have.</p>
<p>说话人 1 33:22<br>2 master notes and 3 worker.Notes also to note here.The hardware resources of Master, an notes servers actually differ.The master processes are more important, but they actually have less load of work so they need less.Resources, like CPU ram and storage whereas the worker nodes do the actual job of running those pods with containers inside.Therefore, they need more resources and as your application complexity.An it’s dim end of resources increases.</p>
<p>说话人 1 33:53<br>You may actually add more master and notes servers to your cluster and thus forming a more powerful and robust cluster to meet your application resource requirements.So, in an existing Kubernetes cluster, you can actually add new master or node servers pretty easily.So if you want to add a master server.You just get a new bear server, you install all the master processes on it.In edit to the covenant is cluster same way if you need 2 worker nodes you get their servers.You install all the worker node processes like container runtime kubelet and Q Proxy and 8 and add it to the covenant is cluster.That’s it and this way, you can infinitely increase the power and resources of your communities cluster is a replication complexity and its resource demand increases.</p>
<p>说话人 1 34:50<br>So, in this video I’m going to show you what mini cube and Cube.Ctl are and how to set them up.So first of all let’s see what is mini cube usually in Kubernetes world when you’re setting up a production cluster?It will look something like this so you would have multiple Masters.At least 2 in a production setting and you would have multiple worker nodes and Masternodes and worker nodes have their own separate responsibility.So as you see on the diagram, you would have actual separate virtual or physical machines that each represent an note.</p>
<p>说话人 1 35:27<br>Now, if you want to test something on your local environment or if you want to try something out very quickly for example.Deploying new application or new components.You want to test it on your local machine.Obviously setting up a cluster like this will be pretty difficult.Or maybe even impossible if you don’t have enough resources.Like memory and CPU, etc and exactly for that use case.</p>
<p>说话人 1 35:53<br>There is this open source tool that is called a mini cube So what a mini cube?Is is basically one node cluster where the master processes.And the worker processes both run on OneNote and this node will have a docker container runtime pre installed.So you will be able to run the containers or the pods with containers.On this note in the way it’s going to run on your laptop is through a virtual box or some other hypervisor.So basically mini cube will create a virtual box on your laptop and the nodes.That you see here is this node will run in that virtual box.So to summarize Mini Cube is a OneNote communities cluster that runs in a virtual box.On your laptop, which you can use for testing Kubernetes on your local setup so now that you’ve set up a cluster or a mini cluster on your laptop or PC on your local machine.You need some way to interact with a cluster.So you want to create components configure it etc.And that’s where Cube Cityel comes in the picture.So now that you have this virtual node on your local machine that represents Mini Cube.You need some way to interact with the cluster so you need a way to create pods.Ain other combinators components on the note.And the way to do it is using Cube City, L, which is a command line tool for Kubernetes cluster so let’s see how it actually works.Remember we said that mini cube runs.</p>
<p>说话人 1 37:26<br>Both Master and work processes so one of the master process is called?Api server is actually the main entry point into the carbonated cluster so if you want to do anything in the Kubernetes If you want to configure anything create any component.You first had to talk to the API server and the way to talk to the API server.Is through different clients so you can have a UI like a dashboard you can talk to it using Kubernetes API or a command line tool, which is Cube City.L&amp;qctl is actually the most powerful of all the 3 clients.Because with capacity L you can basically do anything in the communities that you want and throughout this video tutorials, where you’re going to be using cube CTL mostly so once the cube.Ctl submits commands through the API server to create components delete components.Etc.The work processes on mini cube nodes will actually make it happen.So they will be actually executing the commands to create the pods to destroy the parts to create services etc.So this is the mini cube setup and this is how cube city.L is used to interact with the cluster an important thing to note here is that keeps a TL isn’t just for Mini Cube cluster if you have a cloud cluster aura.Hybrid cluster whatever keeps it here is the tool to use to interact with any type of Kubernetes cluster setup so that’s important to note here.</p>
<p>说话人 1 38:51<br>So now that we know what mini cube and keeps a TLR.Let’s actually install them to see them in.Practice.I’m using Max so the installation process will probably be easier, but I’m going to put the links to the installation guides in the description.So you can actually follow them to install it on your operating system just one thing to note here is that?Mini cube needs a virtualization because as we mentioned it’s going to run in a virtual box set up or some hypervisor so you will need to install some type of hypervisor could be VirtualBox.I’m going to install a hyper kid but it’s going to be in those step by step instructions as well.So I’m going to show you how to install it on a Mac.So I have a Mac OS, Mojave, so I’m going to show you how to install mini cube on these Mac OS version and I’m going to be using brew to install it so update.And the first thing is that I’m going to install.</p>
<p>说话人 1 39:53<br>A hypervisor hyper kit.So I’m going to go with the hyper kit.Go ahead and install it.I already had him install it so with you if you’re doing, it for the first time it might take longer because it has to download all these dependencies and stuff, and now I’m going to install Mini Cube.</p>
<p>说话人 1 40:22<br>And here’s the thing many cube has cube city L as a dependency.So when I execute this, it’s going to install cubes ETL as well.So I don’t need to install it separately.</p>
<p>说话人 1 40:38<br>So let’s see here.Installing dependencies for Mini Cube, which is Kubernetes CLI.This is cube.Ctl again because I already had it installed before it still has a local copy of Dependencies That’s why it’s.Pretty fast, it might take longer if you’re doing, it for the first time so now that everything is installed.It’s actually check the commands so cube.Ctl command should be working, so I get this list of the commands with qol so it’s.There, an mini cube should be working as well, and as you see many cube comes with this command line tool, which is pretty simple so with one command.It’s going to bring up the whole combinators cluster in this OneNote.Set up and then you can do stuff with it and you can just stop it or delete it.It’s pretty easy.</p>
<p>说话人 1 41:30<br>So now that we have both installed and the commands are there?Let’s actually create a mini cube communities cluster?And as you see, there is this start command.Let’s actually clear this so this is how we’re going to start a carbonated cluster cute mini cube start and here is where the hypervisor installed comes in.Because this mini cube needs to run in virtual environment.We’re going to tell many cube which hypervisor it should use to start a cluster, so for that.We’re going to specify an option, which is.Vm driver.</p>
<p>说话人 1 42:07<br>And here I’m going to set the hyper key that I installed so I’m telling Me Cube.Please use hyper kit hypervisor to start this virtual mini Kube Cluster.So when I execute this, it’s going to download some stuff.So again it may take a little bit longer if you’re doing for the first time.</p>
<p>说话人 1 42:28<br>And is a mentioned Mini Cube has docker runtime or Docker Daemon preinstalled so even if you don’t have Docker on your machine.It’s still going to work so you will be able to create containers inside.Because it already contains soccer, which is a pretty good thing.If you don’t have docker already installed so done.</p>
<p>说话人 1 42:49<br>Qctl is now configured to use Mini Cube, which means the Mini Kube cluster is set up.Ioncube CTL Command, which is meant to interact with the carbonated clusters is also connected with that Mini Kube cluster, which means if I.Do cube city will get notes, which just gets me a status of the notes of the companies cluster.It’s going to tell me that the Mini Cube note is ready, and as you see it.It’s the only node an it has a must roll because it obviously has to run the master processes.I can also get the status with Mini Cube.Executing miniclip status so I see host is running kubelet, which is a service that actually runs the pods using container runtime is running so basically everything is running.</p>
<p>说话人 1 43:44<br>And by the way if you want to see Kubernetes architecture in more detail and to understand how master and worker nodes actually work and what all these processes are I have a separate video that covers communities architecture.So you can check it out on this link.And we can also check which version of Kubernetes.We have installed and usually it’s going to be the latest version, so with Q City old version, you actually know what the client version of communities is an what the server version of Kubernetes.And here we see we are using one dot 17 and that’s the Kubernetes version that is running in the mini Kube cluster.So if you see both client version and server version in the output it means that Mini Cube.Is correctly installed so from this point on?We are going to be interacting with the mini kube cluster using cube CTL Command line tool so Mini Cube is basically just for the start up an for deleting the cluster.Everything else configuring we’re going to be doing through Cube Citycell and all these commands that are executed here.I’m going to put them in at least in the comment section so you can actually copy them.In this video I’m going to show you some basic cube city of commands and how to create in debug pots in Mini Cube.So now we have a mini Kube Cluster, an cube CTL installed and once the cluster is set up you’re going to be using cube CTL to basically do anything in the cluster to create components.To get the status etc.So first thing we’re going to just get the status of the notes.So we see that there is OneNote which is a master and everything is going to run on that node because it’s a Mini Cube.</p>
<p>说话人 1 45:34<br>So we keep still get I can check the pots and I don’t have any that’s why no resources.I can check the services keeps detailed kit services.And I just have one default service and so on, so this keeps the tailgate.I can least any comments components.So now since we don’t have any parts were going to create one and to create Kubernetes components there is.The Cube Cityel creates comment so if I do.Help on that cube city create command, I can see available commands for it.So I can create all these components using Q City will create but there is no part on the list.Because in Kubernetes world, the way it works is that part is the smallest unit of the Kubernetes cluster, but usually in practice, you’re not creating parts or you’re not working with the parts directly.There is an abstraction layer over the parts that is called Deployement.So this is what we’re going to be creating and that’s going to create the pods underneath and this is a usage of qol create.</p>
<p>说话人 1 46:47<br>Deployment so I need to give a name of the deployment and then provide some options and the option that is required is the image because the part needs to be created based on certain some image or some container image so let’s actually go ahead and create.</p>
<p>说话人 1 47:03<br>Engine X deployment.So keep city oh create deployment.We let’s call it engine X deployment.Image equals engine X is just going to go ahead and download the latest engine X image from.Docker hub that’s how it’s going to work so when I execute this.You see deployment engine X people created so now if I do Coop City, oh get deployment.You see that I have one deployment created I have the status here, which says it’s not ready yet.So if I do cube CTL get pot.You see that now I have a pot, which has a prefix of the deployment and some random hash here and it says container creating so it’s not ready yet.So if I do it again it’s running.</p>
<p>说话人 1 48:05<br>And the way it works here is that when I create a deployment.Deployment has all the information or the blueprints for creating the pod.The The This is the minimalistic or the most basic configuration for deployment.We just saying the name and the image.That’s it the rest is just defaults and between deployment and a pool.</p>
<p>说话人 1 48:29<br>There is another layer, which is automatically managed by Kubernetes deployment called replica set.So if I do cube cityel get replica set.Written together.You see I have an engine X Dippel replica set hash and it just gives me a state an if you notice here, the pod name has.A prefix of deployment in the replica sets ID and then its own ID, so this is how the pod name is made up and the replica said basically is managing.The replicas of a pod you in practice will never have to create replica set or delete.A replica set or update in any way you’re going to be working with deployments directly.Which is more convenient because in deployment you can configure the pod blueprint completely you can say how many replicas of the pod.You want and you can do the rest of the configuration there here with this command.We just created.One pot or one replica, but if you wanted to have 2 replicas of the engineers spot.We can just provide as additional options.So this is how the layers work first you have.</p>
<p>说话人 1 49:52<br>The deployment the deployment manage is a replica set replica set manage is all the replicas of that pot and the pod is again an abstraction of a container.And everything below the deployment should be managed automatically by Kubernetes usually have to worry about any of it.For example, the image that it uses I will have to edit that in the deployement directly and not in the pot.So let’s go ahead and do that right away, so I’m going to keep CTL edit deployment.And I’m going to provide the name Phoenix.</p>
<p>说话人 1 50:33<br>An we get an auto generated configuration file of the deployment because in the command line.We just gave 2 options.Everything else is default in auto generated by Kubernetes.And you don’t have to understand this now, but I’m going to make a separate video where I breakdown.</p>
<p>说话人 1 50:49<br>The configuration file and the syntax of the configuration file for now, let’s just go ahead and scroll to.The image which is somewhere down below.And let’s say I want to fixate the version 2116.And save that change.And if you see deployment was edited and now when I do coop city El get pot.I see that the old pot.So this one here is terminating an in other one started 25 seconds ago, so if I do it again.The old part is gone and the new one got created with the new image.And if I do if I get replica set.I see that the old one has no parts in it.And a new one has been created as well.So we just edited the deployment configuration and everything else below that.Got automatically updated and that’s the Magic of Kubernetes and that’s how it works.</p>
<p>说话人 1 52:07<br>Another very practical command is Cube City are logs, which basically shows you what the application running inside the pod actually locked so if I do cube sit here logs.And I will need the pod name for this.I will get nothing because engine next didn’t log anything so let’s actually create another deployment.From Mongo DB so let’s call it mongo deployment.And image and image will be one go so let’s see.City oh.</p>
<p>说话人 1 52:48<br>Part so now I have the mongo DB deployment, creating so let’s go ahead and log.That.This state is here means that part was created, but the container inside the pod isn’t running yet and when I try to log.Obviously it tells me there is no container running so it can.Show me and it looks so let’s get the status again.</p>
<p>说话人 1 53:13<br>At this point if I’m seeing that containers and starting I can actually get some additional information by.Keeps detail describe pod and pod name.Which here shows me?What state changes happened inside the pot so it pulled the image created container and started container so cube city will get pod.It should be running already.So now let’s log it keeps ETL logs.And here we see the log output, so we took a little bit, but this is what the market would be application.Container actually locked inside the pot and obviously if container has some problems.It’s going to help with debugging to see what the application is actually printing.So let’s clear that.And get the parts again.</p>
<p>说话人 1 54:09<br>So, in other very useful command when debugging when something is not working or you just want to check what’s going on inside the pod.Is keep cityel exec?So basically what it does is that it gets the terminal of that.Mongo DB application container.So if I do keep city aleksic interactive terminal that’s what it stands for.I will need the pod name.Dash dash?So so with this command.I get the terminal of the mongo DB application container and as you see here.I am inside the container of mongo DB as a root user so I’m in a completely different setting now and as I said, This is useful in debugging.Or when you want.Test something or try something you can enter the container or get the terminal and execute some comments.Inside, there, so we can exit that again.</p>
<p>说话人 1 55:12<br>And, of course, with Coop City, El I can delete the pots.So if I do get deployment.I misspelled it so keeps it here employment.I see that I have 2 of them and if I do.</p>
<p>说话人 1 55:29<br>Qc pod and replica said.I have also 2 of them so let’s say if I wanted to get rid of.All the pods replica sets underneath I will have to delete.</p>
<p>说话人 1 55:40<br>The deployment.So delete deployment and I’ll have to provide the name of the deployment.I’m going to delete it.Delete Mongo DB.Delete it and now if I’m going to say keep city.I’ll get pod.The pod should be terminating an if I do get replica set.The Mongo DB replica set is gone as well in the same if I do delete deployment.</p>
<p>说话人 1 56:12<br>Engine X Temple.Undo the replica said.See everything gone so all the crud operations create delete update etc.Happens on the deployment level and everything underneath just follows automatically and the similar way way we can create other community resources like services, etc.</p>
<p>说话人 1 56:33<br>However, as you notice when we’re creating current components like deployment using cube cityel create deployment.I misspelled it all the time.You’ll have to provide all these options on the command line.So you’ll have to say the name and you have to specify the image and then you have this option, one option, 2, etc.And there could be a lot of things that you want to configure in a deployment or in a pot and obviously it will be impractical to write that all out on a command line zero because of that in practice.You would usually work with communities configuration files, meaning what component you’re creating what the name of the component is what image is it based off in any other options there.All gathered in a configuration file and you just tell Coop City.</p>
<p>说话人 1 57:30<br>El to execute that configuration file and the way you do it is using cubes ETL apply command.And apply basically takes the file the configuration file as a parameter and does whatever you have written there, so apply.It takes an option called minus F let’s dance for file.And here you would say the name of the file so this will be the config file dot yellow.This is the format that usually going to use for configuration files and this is the command that.Executes whatever is in that configuration file so let’s actually called it configuration file.</p>
<p>说话人 1 58:14<br>Engine next deployment.And let’s go ahead and create a very simplistic super basic engineering deployment file so here I’m going to.Create that file.So this is the basic configuration for the deployment.So here I’m just specifying what I want to create I want to create a deployment.The name of that deployment.</p>
<p>说话人 1 58:46<br>You can ignore this labels right now?How many replicas of the pods.I want to create an these plug right here that template and specification is a blueprint.Or the pods so specification for the deployment and specification for a pod and here we’re just saying that we want one container inside of the pod with engine X image.And we’re going to find that on port 80.So this is going to be our configuration file and once we have that we can apply that configuration.So.So deployment created somehow if I get part.I see that engine X deployment pod was created an it’s running and it’s also seated.Deployement was created 52 seconds ago and now if I wanted to change something in that deployment.I can actually change my local configuration.For example, I want it 2 replicas instead of one.I can apply that.Again.Deployment engineers deployment configured and as you see the difference here is that?Communities can detect if the engine X deployment doesn’t exist.Yet it’s going to create one.But if it already exists and I play the configuration file again.It’s going to know that it should update it instead of creating a new one so if I do get employment.</p>
<p>说话人 1 01:00:32<br>I see this is the old one or the old deployment.And if I do.Cube CTL get pod.I see the old one is still there and a new one got created because I increased the replica count, which means that with cube city apply you can both create.And update a component and obviously you can do coops ETL with services.</p>
<p>说话人 1 01:00:54<br>Volumes any other combinators components just like we did it with the deployment so in the next video.I’m going to breakdown.The syntax of the configuration file which is pretty logical and simple actually to understand and I’m going to explain all the different.Attributes and what they mean so you can write your own configuration files for different components.So to summarize.</p>
<p>说话人 1 01:01:15<br>We’ve looked at a couple of Coops.Etl comments in this video.We saw how to create a component like deployement.How to edit it and delete it.We saw how to get status of parts deployments replica sets etc?We also logged on console.Whatever application is writing it to the console in the pod.And we saw how to get a terminal of a running container using cubes.Etl Exec and finally.</p>
<p>说话人 1 01:01:43<br>We saw harder use communities configuration file to create an update components.Using the cube city apply command and last but not least, we saw Cube City of describe command, which will winner containers and starting in a pot and you want to get some additional troubleshooting information about the pod.</p>
<p>说话人 1 01:02:07<br>In this video I’m going to show you the syntax and the contents of Kubernetes configuration file which is the main tool for creating and configuring components in communities cluster if you’ve seen lar.</p>
<p>说话人 1 01:02:20<br>Configuration files it might seem overwhelming, but in reality.It’s pretty simple and intuitive and also very logically structured so let’s go through it step by step.So here I have examples of a deployment and service configuration files side by side, so the first thing is that every configuration file in Kubernetes has 3 parts.The first part is where the metadata of that component that you’re creating results.One of the metadata is obviously name of the component itself, the second part in the?Configuration file is specification, so each component configuration file will have a specification where you basically pulled every kind of configuration that you want to apply for that.Um components the first 2 lines here is you see is just declaring what you want to create here.We are creating deployment in here.We’re creating a service and this is basically you have to look.For each component there is a different API version.</p>
<p>说话人 1 01:03:31<br>So now inside of the specification part.Obviously, the attributes will be specific to the kind of a component.That you are creating so deployment will have its own attributes that only apply for deployment and the service will have its own stuff.But I said there are 3 parts of a configuration file.And we just see metadata and the specification.So where’s the third part so the third part will be a status, but it’s going to be automatically generated and added by Kubernetes.So the way it works is that communities will always compare.What is the desired state and what is the actual stated or the status of their component and if the status and desired state do not match.</p>
<p>说话人 1 01:04:21<br>Then commanded his nose.There’s something to be fixed, there, so it’s going to try to fix it.And this is the basis of the self healing feature that Kubernetes provides for example, here you specify you want.</p>
<p>说话人 1 01:04:35<br>2 replicas of Engineers deployment so when you apply this when you actually create the deployment using this configuration file that’s what ally means committees will add here, the status.Of your deployement and it will update that states continuously so for example, if a status at some point will say just one replica is running then communities will compare that status with the specification.And will know there is a problem there.Another replica needs to be created ASAP now.</p>
<p>说话人 1 01:05:08<br>Another interesting question here is where does communities actually get the status data to automatically add here.Or update continuously that information comes from the IT CD.Remember the cluster brain.One of them must processes that actually stores.The cluster data so it’s Cindy Holt sat.Anytime the current status of any communities component and that’s where the status information comes from.</p>
<p>说话人 1 01:05:40<br>So as you see the format of the configuration files is yamil that’s why the extension here and generally it’s pretty straightforward to understand it’s very simple format, but Yamil is very strict about the.Indentations so for example, if you have something wrongly in.I did hear your file will be invalid So what I do, especially if I have a configuration file that has 200 lines.It’s pretty long.I usually use some yellow online validator.To see where I need to fix that, but other than that, it’s pretty simple and another thing is where do you actually store those configuration files are usual practice is to store them with your code?Because since the deployment in service is going to be applied to your application.It’s a good practice to store these configuration files in your application code so usually it will be part of the whole.Infrastructure as a code concept or you can also have its own git repository just for the configuration files.</p>
<p>说话人 1 01:06:50<br>So, in the previous video, I showed you that deployements manage the parts that are below them.So, whenever you edit something in a deployment.It kind of Cascade style down to all the ports that manage is.And whenever you want to create some parts.You would actually create a deployment and it will take care of the rest.</p>
<p>说话人 1 01:07:11<br>So how does this happen or where?Is this whole thing defining the configuration?So here in the specification part of a deployement you see template.And if I expanded you see the template also has its own metadata.An specification so it’s basically a configuration file inside of a configuration file.And the reason for it is that this configuration.Applies to a pod so part should have its own configuration inside of deployments configuration file and that’s how all the deployments will be defined and this is going to be the blueprint.For a pot like which image it should be based on which port it should open.What is going to be the name of the container etc?</p>
<p>说话人 1 01:08:05<br>So the way the connection is established is using labels and selectors, so as you see metadata part contains the labels.And the specification part contains selectors, it’s pretty simple in a metadata.You give components like deployment or pod.A key value pair and it could be any key value pair that.You think of this case, we have app engine X and that label just sticks to that component so we give pods.Created using this blueprint label engine X and we tell the deployement to connect or to match all the labels.With app engine X?To create that connection.So this way deployment will know which parts belong to it now deployment has its own label app engine.X and these 2 labels are used by the service selector zero in the specification of a service.We define a selector, which basically makes a connection.Between the service and the deployment or it spots because service must know which parts are.Kind of registered with it, so which pods belong to that service and that connection is made through this selector.Off the label and we’re going to see that.The demo so another thing that must be configured in the service and pod is the ports so if I expand this.I see that service has its ports.Configuration and the container inside of a part is obviously running or needs to run its import right so how this is configured is basically service has a port.Where the service itself is accessible at so if other services and the request to engineering service here in needs to send it on port 80 but this service needs to know.To which part it should forward the request, but also at which port is that part listening.And that is the target port so this one should match the container port.</p>
<p>说话人 1 01:10:33<br>And with that, we have our deployment and service basic configurations done and to note here.Most of these attributes that you see here in both parts are required so this will actually be the minimum.</p>
<p>说话人 1 01:10:48<br>Configuration for deployment and service so once we have those files.Let’s actually apply them or create components using that so let’s head over to the console and here I’m going to create both deployment and service.So keep city apply.Genetics deployment.Created and engine X service so now if I.Get the pods I see 2 replicas are running because that’s how we define it here and we have our service as well.</p>
<p>说话人 1 01:11:30<br>Which is engineering service this is a default service?It’s always there this is the one we created?And it’s listening on port 80 as we specified now.How can we validate that the service has the right parts that it forwards the?Requests to we can do it using cube city old describe service and the service name.</p>
<p>说话人 1 01:12:01<br>And here you see the endpoints.Where you have all these status information here like things that we define in the configuration like selector, etc.We have the target port that we define an we have the?Endpoints here and this must be the IP addresses.Imports of the pots.That the service must forward.The request to so how do we know that these are the IP addresses of the right parts because we’ve keeps the tailgate pod.You don’t get this information.So the way we do it or we find that out, is using?Get part and then you do dash.Oh, which is 4 Outputs and then we want.More information so all white.</p>
<p>说话人 1 01:12:55<br>And here we see more columns here, so we have the name and status ready, etc.But we also have the IP address.So here is the IP address endpoint specified here and this is.The other one so we know that the service has right endpoints.</p>
<p>说话人 1 01:13:13<br>So now let’s see the third part of the configuration file which is a status that Kubernetes.Magically generated and the way to do it is, we can get the employment.</p>
<p>说话人 1 01:13:27<br>Engine X deployment in a Yaml format.So when I execute this command.I will get the resulting or the updated configuration of my deployment, which actually resides in the LCD.Because it’s in the stores the status of the whole cluster, including every component.So if I do this.I’ll get the yaml output in my console, but I wanted in the file so I’m going to.</p>
<p>说话人 1 01:13:54<br>Saving into engine X?The employment.Result.And I’m going to save it there and I’m going to open it in my editor next to the original 10 as you see a lot of stuff has been added, but let’s just see the status part.So all this is automatically added an updated constantly by Kubernetes.So it says how many replicas are running what the state of those replicas and some other information.So this part can also be helpful when debugging so that’s the status but also, if you notice.</p>
<p>说话人 1 01:14:40<br>Other stuff has been added in the metadata and specification part as well so for example, creation timestamp.When was the component created is automatically added by communities?Because it is a metadata some unique ID, etc.You don’t have to care about it and in the specification part.It’s just it’s some defaults for that component, but again, you don’t have to.Care one stand most of these attributes but one thing to note here is that if you for example, want to copy deployement that you already have using maybe automated scripts.You will have to remove and get rid of most of these generated stuff.So you have to clean that deployment configuration file first and then you can create another deployment.From that blueprint configuration so let’s it with this video so from now on.We’re going to be working with the configuration files so for example, if I want to delete the deployment and service.I can do it using that file.Configuration file is well using delete.And.Like this.The deployment will be gone and I can do the same for service.Alright so using cube CTL apply and keeps it will delete you can basically work with the configuration files.</p>
<p>说话人 1 01:16:19<br>In this video we’re going to deploy 2 applications.Mongo DB in Mongo Express and I chose these 2 because it demonstrates really well.A typical simple setup of a web application and database.So you can apply this to any similar setup, you have so let’s see how we are going to do this.So first we will create a mongo DB pod and in order to talk to that part we’re going to need a service.An we’re going to create an internal service, which basically means that no external requests are allowed to the pod.Only components inside the same cluster can talk to it, and that’s what we want then we’re going to create a Mongo Express.Employment one we’re gonna need database URL of mongo DB so that Mongo Express can connect to it, and the second.One is credentials.So username and password of the database so that it can authenticate.</p>
<p>说话人 1 01:17:13<br>So the way we can pass this information to Mongo Express deployment is through its deployment configuration file through environmental variables because that’s how the application is configured so we’re going to create a config map.That contains database URL and we’re going to create a secret that contains the credentials and we’re going to reference both inside of that deployment file so once we have that set up we’re going to need Mongo Express to be accessible.Through browser in order to do that.We are going to create an external service that will allow external requests to talk to the pot.So the URL will be patient P IP address of the.</p>
<p>说话人 1 01:17:54<br>Note ends the service port so with this setup, the request flow will now look like this so the request comes from the browser and it goes to the external service of the Mongo Express, which will then forward it to the Mongo Express.Pod the pod will then connect to internal service of Mongo.Db that’s basically the database URL here and it will forward it then to monkey pod, where it will authenticate.The request using the credentials so now let’s go and create this whole setup using Kubernetes configuration files.Let’s dive right into it and create the whole setup so first of all I have a mini Kube cluster running if I do.Cube CTL gets all which basically gets me all the components that are inside the cluster.I only have a default currently service so my cluster is empty an I’m sorry from scratch.So the first thing that I said, We’re going to do is create a mongo DB deployment usually created in and.Editor so I’m going to go to Visual Studio Code and paste prepared deployment file there.For Mongo DB and this is how it’s going to look like so I have deployment kinds.And I have some metadata.I’m just going to call it mongo DB deployment.Labels and selectors in the previous video.I already explained the syntax of Kubernetes.Yaml configuration file so if you want to know what all these attributes mean then you can check out that video.And here in the template.I have a definition or blueprint for parts that this deployment going to create an I’m just going to go with one replica so the container is going to be called Mongo DB.And this is the image that I’m going to take so let’s actually go and check out the image configuration for Mongo DB.So Mongo.And I see.This image here, let’s open this.And basically what I’m looking for is how to use that container, meaning what ports it’s going to open an what’s external configuration is going to take so a default?</p>
<p>说话人 1 01:20:17<br>Port of mongo DB container is 27,017, so I’m going to use that.And we’re going to use variables environmental variables.The root username and root password.So basically I can on the container startup define the admin username and password.So let’s go ahead and.Figure all of that inside the configuration file so here below the image among the beat.So we’re just going to leave the name of the image and it’s going to pull the latest one and that’s what we want.</p>
<p>说话人 1 01:20:53<br>So here I’m going to specify what port I want to expose soportes attribute name and container.Port.And that’s the standard port so I’m going to leave it and below that I’m going to specify those 2 environmental variables, so one is called Let’s see what it’s called?Mongo init DB root username and here is going to be a value.So we’re going to actually leave it blank for now, and the other one is.Cold init root password and we’re going to leave that blank as well, just bad you and once we have the values here.</p>
<p>说话人 1 01:21:40<br>We’re going to have a complete deployment for Mongo DB.This is basically all we need now note that this is a configuration file that it’s going to be checked into a repository so usually you wouldn’t write admin username and password inside the configuration file.So what we’re going to do now is we’re going to create a secret from where we will reference.The values so meaning that the secret is going to leave in Kubernetes and nobody will have access to it in a git repository.So we’re going to save these incomplete deployment file first of all, so let’s call it mongo.</p>
<p>说话人 1 01:22:21<br>Deployment.Or let’s just call it Monroe Yamo and save it here, so that we get the syntax highlight and now before we apply this configuration.We’re going to create the secret where the root username and password will leave.So let’s create a new file and I’m going to paste in the configuration of a secret which is actually pretty simple.So we have a kind secret then we have a metadata, which again is just simply the name.</p>
<p>说话人 1 01:22:53<br>We’re going to create mongo DB secret the type or Peck is actually a default type, which is the most basic key value secret type.Other types for example, include TLS certificates, so you can create a?Secret specifically with the TLS certificate type Anna couple of more types, but mostly you’re going to use the default.One and these are the actual contents.So we have the data and here you have key value pairs.Which of course are the names you come up with so we’re going to specify username or we can actually call it mongo.</p>
<p>说话人 1 01:23:31<br>Route.User name and we’re going to call it Mongo Root password and here’s the thing the values in.In this key value pairs are not plaintext.So when we are creating a secret.The value must be base 64 encoded so the way you can do that the simplest way is go to your terminal.So here I’m going to say Echo minus N very important option don’t leave it out.Otherwise, it’s not going to work.And here I’m going to put a plaintext value, then I want so I’m just going to go with just username.</p>
<p>说话人 1 01:24:08<br>Whatever of course, you can have something more secretive here and I’m going to base 64 Encoding and the value that I get here.I’m going to copy it into the secret configuration as a value and I’m going to do the same with password so again.I’m just going to go with simple password.Obviously, you want to have something more secure.And I’m going to copy that is a value here and save it is Mongo Secret Dot HTML.</p>
<p>说话人 1 01:24:46<br>Ok, now we have only written configuration files.We haven’t created anything yet.In the cluster so this is just preparation work and we have to create secret before the deployement if we’re going to reference the secret.Inside of this so the order of creation matters because if I’m creating a deployement that references a secret that doesn’t exist yet.</p>
<p>说话人 1 01:25:09<br>I’m going to get an error.So it’s not going to start since we have our first component.Let’s actually go ahead and create our secret.From the configuration file so again I’m going to go to my console.It’s actually clear.All this and I’m going to go into the folder where I’m creating all these configuration files.I called it quits configuration and here I have both of my files so I’m going to keep city apply Mongo Secret.And secret created so I’m going to do cube CTL get secret.</p>
<p>说话人 1 01:25:48<br>An I should see my secret has been created.This is something created by default with a different type and this is our secret here.So now that we have our secrets.We can reference it inside over deployment configuration file so let’s go back and this is how you reference contents specific key value.Data of secret, so is there a value.We’re going to say value from.And then I’m going to do secret key ref.Secret key reference.Ann.Name is going to be the secret name so this one here.And key is going to be the key in the data.I want the value of these key value pair.So I want this part of the data.So I’m going to reference it by key so you don’t have to learn it by heart.Obviously all the syntax and attribute names important thing here is that you know approximately how to reference it.The actual syntax.You can always look up in Google or maybe from previous configuration files.But yeah, this is how you reference it and we’re going to do the same with password.So I’m going to do from an I’m just going to copy the rest here.Remember yellow is very strict with the indentation here is the same secret but a different key so I’m going to use password key here.And.That will be it so now we have.The root username and password referenced from the secret and no actual values inside the configuration file which is good for security because you don’t want your credentials in your code repository.Ok, so our deployment file is actually ready so let’s apply that.Ship city out there play.</p>
<p>说话人 1 01:27:55<br>And the deployment created, meaning if I do get all.I should see the pod starting up the deployment and the replica set so let’s actually.Check how hot is doing.Container creating so let’s actually watch it might take some time to create it if it takes long and if you want to see whether there is a problem there.You can also do cube CTL describe pod.In the pod name.So at least we know nothings wrong, there, so we see that it’s just pulling the image so that’s what it takes so long.So let’s see again cube city will get pot and as you see it’s running so we have multiple deployment and the port.One replica of its part running.</p>
<p>说话人 1 01:28:48<br>Now the second step is we’re going to create an internal service.So that other components or other ports can talk to these mongo DB so let’s go ahead and create service configuration so go back to Yemen.And here we can either create a separate yelmo configuration file for secret.Or we can also include it in the same one so in general.You can actually put multiple documents in one file so if I put 3 dashes.That’s basically a syntax for document separation in Yemen.So I need new document is starting so actually I’m going to put both deployment and service in one configuration file because they usually belong together, so here I’m going to.Paste the service configuration and by the way I’m going to put all these configuration files in Git repository and link.The repository in the description of this video so this is a service for Mongo DB.</p>
<p>说话人 1 01:29:46<br>Let’s go through some of the attributes here, so it’s the service kind just the name.We’re going to call it multi service selector.This is an important one because we want this service to connect to the pod right.And the way to do that is using selector and label so using this here.The labels that deployment and part have service can find the pods that it’s going to attach to.Alright so we have the selector here and this is important part where we expose service port so this is going to be the service port and this is going to be the container and since we expose container.</p>
<p>说话人 1 01:30:28<br>Port it this address but here these 2 have to match so target port is container or pot port and this is the service port and obviously these 2 here.Can be different but I’m going to go with the same port and that’s basically it that’s our service, so I’m going to create the service now.So let’s save this file and go back to my.Consult an I’m going to apply.The same file that I applied before to create deployment.So let’s see what happens see both deployment and service configuration, but it’s going to know that I haven’t changed deployment.That’s what it means here and.</p>
<p>说话人 1 01:31:13<br>Services created so if I were to edit both for example, I can reapply the file and diplomatic service can be changed.So I think using local configuration files is a handy way too.</p>
<p>说话人 1 01:31:27<br>Edit your components so now let’s actually check that our service was created get service.And this is our service and it’s listening at port.27,017 and I showed it in one of the previous videos.But we can actually also validate that the service is attached to the correct pod and to do that I’m going to describe.</p>
<p>说话人 1 01:31:55<br>Service you know need the service name for this.So here I have the endpoint, which is an IP address of the pod and the port where the application inside the pot is listening.It so let’s actually check that this is the right pot.I mean, we just have one but still.So if I do get pot Ann.I want additional output to what I get by default.One of the columns includes the IP address, which is this one right here so.172 point 17.06.That’s the pot IP address and this is the port where the application inside the pot is listening.So everything set up perfectly.</p>
<p>说话人 1 01:32:44<br>Mongo DB deployment and service has been created.And by the way if you want to see all the components for one.Application you can also display them using cubes.Etl get all that will show all the components and you can filter them by name so Mongo DB.And here you see the service.</p>
<p>说话人 1 01:33:10<br>Deployement replica, said in the pod so when you do all that component type will be first here OK.That’s just a site info.So now the next step.We’re going to create.Mongo express deployment and service and also an external configuration where we’re going to put the database.Url from going to be so let’s go ahead and do it, so I’m going to clear that up and.Go and create a new file for Mongo Express deployment in service.So this is the deployement draft of Mongo Express.</p>
<p>说话人 1 01:33:49<br>Same things here, Mongo Express that’s the name and here we have the pod definition where the image name is Mongo Express.Let’s actually go ahead and check that image as well.We don’t need this, this is more express.And that’s the name of the image more express and let’s see the same data here.Let’s see the port.</p>
<p>说话人 1 01:34:13<br>The Mongo Express application inside the container starts at is 8081.And.These are some of the environmental variables, so obviously, we need 3 things for Manga Express.We need to tell it, which database application.It should connect to so obviously, we need to tell it the Mongo DB.Address database address, it should connect to the internal service and we’re going to need credentials.So that mongo DB can authenticate that connection in the environment variables to do that is going to be.Admin username and password and the mongo DB endpoint will be this year.</p>
<p>说话人 1 01:34:53<br>So this 3 environments variables.We need so let’s go ahead and use that so first we’re going to open the port.Again.Container ports.And the reason why you have multiple ports that inside of the part where you can actually open multiple ports.So that’s going to be 8081 and now we’re going to.</p>
<p>说话人 1 01:35:20<br>At the environmental variables for the connectivity so the first one is.The username and this is going to be.Obviously, the same username and password that we defined right here.So what I’m going to do is I’m just going to copy them because it’s really the same, so the value from we’re going to read it from the secret.That’s already there, so I’m going to paste it here.</p>
<p>说话人 1 01:35:47<br>Second environmental variable is called and password.And I’m also going to copy that from here.And the third one.He’s gonna be the database server, and since this is also an external configuration.We could either do value here and we could write the mongo DB server address directly here.Or as I showed you in the diagram at the beginning, we can put it in a config map, which is an external configuration, so that it’s centralized so it’s stored in one place and also other components can also use it.So for example, if I have 2 applications that are using mongo DB database.Then I can just reference that external configuration here and if I have to change it at some point I just change it in one place.And nothing else gets updated so because of that.We’re going to keep this incomplete deployment configuration and we’re going to create the config map, which will contain the mongo DB server.</p>
<p>说话人 1 01:36:55<br>Address so I’m going to create a new file that’s actually save.This incomplete deployment.Let’s call it Mongo Express gamelan.We’re going to come back to it later.So save that now we need a config map here, so I’m going to copy the configuration and this is also pretty simple.</p>
<p>说话人 1 01:37:16<br>Just like secret you have the kind.Which is config map name end the same construct see just like you saw here data?Which is key value pair it doesn’t have a type.Because they’re just one config map type and that’s it.And here you again have key value pairs.</p>
<p>说话人 1 01:37:37<br>So database URL and server name is actually the name of the service.It’s as simple as that.So what do we call our service.We called it mongo DB service.So I’m going to copy the service name and that’s going to be the database server.</p>
<p>说话人 1 01:37:53<br>Url so I’m going to copy that and let’s actually call it mongo.Config map for consistency and save it and just like with secret the order of execution or creation matters.So I have to have a config map already.In the cluster so that I can reference it so when we’re done.I have to can create the config map first and then the deployment so the way that I can reference.The config map inside the deployment is very similar to secret, so I’m actually going to copy the whole thing.From secret put it here.The only thing different here is that instead of secret I’m going to say.Config.Map it’s all camel case and obviously the name is going to be.</p>
<p>说话人 1 01:38:46<br>Config map that’s what we called it I think.Yes, that’s the name that’s actually copied and again.The key is the key in the key value pair here so let’s copy that as well.So now we have our Mongo Express deployment.These are just standard stuff and this is where the pod blueprint or container configuration exists.We have exposed port 8081 this is the image.With latest tag and these are the 3 environmental variables that Mongo Express needs to connect and authenticate with mongo DB so deployment is done and let’s go ahead and create.Config map first and then Mongo Express deployment.Ctl apply?Config map.</p>
<p>说话人 1 01:39:41<br>And I’m going to do conceal apply.We Express.And let’s see the pot.So container creating.Looks good so see the pod.And it’s running.An I actually want to see the Lux so I’m going to lock that, Mongo Express.And here you see that express service started and database connected.</p>
<p>说话人 1 01:40:14<br>So now the final step is to access Mongo Express from a browser and in order to do that, we are going to need.An external service for mobile express so let’s go ahead and create that one as well.So it’s clear these output.Go back to visual code Ann as we did last time in the same file as a deployment.</p>
<p>说话人 1 01:40:35<br>I’m going to create more express service because actually in practice.You never have deployment without the service.So it makes sense to keep them together and this is Mongo Express External Service.</p>
<p>说话人 1 01:40:49<br>And this configuration right now looks exactly the same as the Mongo.Db service configuration and even ports are the same, like here, I have exposed service port 8081.And target port is where the container port is listening.</p>
<p>说话人 1 01:41:07<br>So how do I make these external service is by doing 2 things so in this specification?Section so I’m going to do it below the selector I’m going to put a type.In a type of this external services load balancer.Which I think is a bad name for external service because internal service also acts as a load balancer so if I had 2.Mongo DB parts.The internal service would also load balance, the requests coming to these pots.So I think the load balancer type name was chosen not very well because it could be confusing.But what this type load balancer does basically it accepts external requests.By assigning the service an external IP address so another thing that we’re going to do here to make this service.</p>
<p>说话人 1 01:42:05<br>External is right here.We’re going to provide third port and this is going to be called.Note port.And what this is basically is the port where this, external IP address will be open so this will be the port that I’ll have to put in the browser to access this service.In this note port actually has a range and that range is between 30,032 thousand something so I can not give it the?Imports here is I said it has to be between that range.So I’m just going to go with the 30,000 that’s the minimum in that range and that would be it.</p>
<p>说话人 1 01:42:50<br>So this configuration here will create an external service.Let’s go ahead and do it and I will show you.Exactly how this ports differ from each other so I’m going to apply.</p>
<p>说话人 1 01:43:06<br>Express.So service created and if I do get service.I see that mongo DB service that we created previously has a type of cluster IP in the Mongo Express service that we just created is load balancer, which is the type that we specifically defined.An internal service within specify any type because cluster IP, which is same as internal service type is default.So you don’t have to define it when you’re creating internal service.And the difference here is that cluster IP will give the service and internal IP address, which is this one right here.So this is an internal IP address of the service.</p>
<p>说话人 1 01:43:55<br>And load balancer will also give service an internal IP address.But in addition to that it will also give the service.An external IP address where the external requests will be coming from.</p>
<p>说话人 1 01:44:08<br>And here it says pending because we’re in Mini Cube and it works.A little bit differently.In a regular community set up here.You would also see an actual IP address at public one and this is another difference because with?Internal IP address you just have port for that IP address with both internal and external IP addresses.You have ports for both of them and that’s why we had to define third port, which was for the.</p>
<p>说话人 1 01:44:35<br>External IP address this is said pending means that he doesn’t have the external IP address yet so in Mini Cube, the way to do that is using the command Mini Cube.</p>
<p>说话人 1 01:44:46<br>Service and I’m going to need the name of the service.So this command will basically assign external service at public IP address so I’m going to execute this and the browser window will open.And I will see my Mongo Express page.So if I go back to the command line.You see that this command here assigned Mongo Express Service.A URL with the public IP address or with an external IP address.In the port which is what we defined in the node port so I can basically copy that command, which is the same as this one here.</p>
<p>说话人 1 01:45:26<br>And I get the page Formula Express so now with this setup, the way it’s going to work is that when I make changes here for example, I’m going to create a new database.Let’s call a test.Db whenever an I’m going to create a request.What just happened in background is that this request blended with the external service of Mongo Express, which then forwarded it to the Mongo Express.Part in the manga express pod connected to the Mongo.Db Service, an internal service.An mongo DB service, then forwarded that request finally to the mongo DB pot and then all the way back.</p>
<p>说话人 1 01:46:07<br>And we have the changes here.So that’s how you deploy a simple application set up in a Kubernetes cluster.In this video.Going to go through the usages of a namespace and the best practices of when and how to use a namespace.First of all what is a namespace in communities encourages cluster you can organize resources in namespaces so you can have multiple namespaces in a cluster you can think of a name space is a virtual cluster inside of a Kubernetes cluster.Now, when you created cluster by default Kubernetes gives you namespace is out of the box so in the command line.If I Type Cube.Ctl get namespaces.I see the list of those out of the box.Namespaces that Kubernetes offers and let’s go through them one by one.</p>
<p>说话人 1 01:47:01<br>The Kubernetes dashboard.Namespace is shipped automatically in Mini Cube.So it’s specific to mini cube installation, you will not have this in a standard cluster.The first one, is cube system.Kube system namespace is not meant for your use so basically you shouldn’t create anything or shouldn’t modify anything in cube system namespace.The components that are diploid in the namespace or the system.Processes there from master managing processes or cubes ETL, etc.</p>
<p>说话人 1 01:47:34<br>The Next One is Q Public and what Q Public contains is basically the publicly accessible data, it has a config map.That contains cluster information, which is accessible.Even without authentication.So if I type here.Q City of cluster info.This is the output that I get through that.Information and the third one is cube node list, which is actually a recent addition to communities and the purpose of that namespaces that it holds information about the Heartbeats of notes so each node basically.Gets its own object that contains the information about that.</p>
<p>说话人 1 01:48:13<br>Nodes availability and the fourth namespace use the default namespace and default namespace is the one that you’re going to be using to create.The resources at the beginning if you haven’t created a new namespace, but of course, you can add and create new name spaces and the way that you can do it is using cube CTL create namespace command.With the name of the namespace so I can create my name space.And if I do.Coops ETL get namespaces.I see that in my list.</p>
<p>说话人 1 01:48:45<br>Now, another way to create namespaces.Is using namespace configuration file?Which I think is a better way to create namespaces because you also have a history in your configuration file repository of what resources.Rated in the cluster OK, so now we saw what namespaces are and that you can create new ones.And that Kubernetes offers some of them by default, but the question is what is the need for namespaces?When should you create them and how you should use them and the first use case of using or creating your own namespaces is the following imagine you have only default namespace, which is provided by.Communities and you create all your resources in that default namespace.If you have a complex application that has multiple deployments, which create replicas of many parts and you have resources like Services, an config Maps, etc.Very soon your default namespace is going to be filled with different components and it will be really difficult to have an overview of what’s in there, especially we have multiple users, creating stuff inside so a better way to use namespaces.</p>
<p>说话人 1 01:49:59<br>In the.Case is to group resources into namespaces, so for example, you can have a database name space where you deploy your database and all its required resources and you can have a monitoring.</p>
<p>说话人 1 01:50:14<br>Name space where you diploid the parameters and all the stuff that it needs you can also have elastic stack namespace where all the Elasticsearch Kibana, etc resources goal and you can have engine X Ingress resources.So, just one way of logically grouping your resources inside of the cluster now, according to the official documentation of communities.</p>
<p>说话人 1 01:50:39<br>You shouldn’t use namespaces if you have smaller projects.End up to 10 users.I personally think that it’s always a good idea to group your resources in namespaces because as I said, even if you have a small project and 10 users.</p>
<p>说话人 1 01:50:58<br>You might still need some additional resources for application like logging system and money.During system and even with the minimum set up you can already get too much to just throw everything in a default namespace.</p>
<p>说话人 1 01:51:13<br>Another use case where you will need to use namespaces if you have multiple teams.So imagine this scenario, you have 2 teams that use the same cluster in one team.Deployes an application, which is called my app deployment.That’s the name of the deployment they create.And that deployment has its certain configuration now if another team had a deployment that accidentally had the same name.But a different configuration and they created the deployment or they applied it.They would overwrite the first teams deployment and if they’re using for example.A Jenkins or some automated way to deploy those that application or to create the deployment they wouldn’t even know that.They overwrote or disrupted in other teams deployment.So to avoid such kind of conflicts again.You can use namespaces so that each team can work in their own namespace without disrupting the other.</p>
<p>说话人 1 01:52:15<br>Another use case for using namespaces is let’s say you have one cluster and you want to host both staging and development environment in the same cluster in the reason for that is that for example, if you’re using?Something like engine X controller or elastic SEK use for logging for example, you can deploy it in one cluster an use it for both environments in that way.You don’t have to deploy this common resources.Twice in 2 different clusters so now the staging can use both resources as well as the development environment.</p>
<p>说话人 1 01:52:51<br>Another use case for using namespaces is when you use blue green deployment for application?Which means that in the same cluster you want to have 2 different versions of production so the one that is active that is in production now and another one that is going to be the next production version.The versions of the applications in those blue and green production namespaces will be different.However, the same as we saw before in staging and Development.This namespaces might need to use the same resources.</p>
<p>说话人 1 01:53:24<br>Think again answer next controller or elastic stake in this way again.They can both use.This common shared resources without having to set up a separate cluster so one more use case.For using namespaces is too.Limit the resources and access to namespaces when you’re working with multiple teams.</p>
<p>说话人 1 01:53:46<br>So again.We have a scenario where we have 2 teams working on the same cluster and each one of them has their own namespace.So what you can do in this scenario is that you can give.The teams access to only their name space so they can only be able to create updates.Delete resources in their own namespace, but they can’t do anything.In the other namespaces and this way, you even restrict or even minimize the risk of one team accidentally interfering with another teams work so each one has their own secured isolated environment.</p>
<p>说话人 1 01:54:21<br>Additional thing that you can do on the namespace level is limit the resources that each namespace consumes because if you have a cluster with limited resources.You want to give each team share of resources.For their application so if one team.Let’s say consumes too much resources than other teams will eventually have much less and their applications may not schedule because the cluster will run out of the resources.So what you can do is that per namespace.You can define resource quotas that limit how much CPU.</p>
<p>说话人 1 01:54:55<br>Ram storage resources.One namespace can use so I hope walking through this scenarios helped you.Analyze in which use cases and how you should use namespaces in your specific project.There are several characteristics that you should consider before deciding how to group and how to use namespaces.The first one, is that you can’t.Access most of the resources from another namespace, so for example, if you have a configuration map in project.A namespace that references the database service, you can’t use that config map.In Project B namespace, but instead you will have to create the same config map that also references the database service, so each namespace will define or must define its own config map.Even if it’s the same reference in the same applies to secret, so for example, we have credentials of a shared service, you will have to create that secret in each namespace where you are going to need that however.A resource that you can share across namespaces is service and that’s what we saw in the previous slide.So config map in Project B namespace references service that is going to be used eventually in a pot.And the way it works is that in a config map definition that database URL.In addition to its name, which is my SQL service will have namespace at the end, so using that URL you can actually access services.From other namespaces, which is a very practical thing and this is how you can actually use shared resources like Elasticsearch or engine X from other namespaces and one more characteristic is that?</p>
<p>说话人 1 01:56:40<br>We saw that most of the components resources can be created within a namespace.But there are some components in Kubernetes.They’re not namespaced so to say basically they leave just.Globally, in the cluster and you can’t isolate them or put them in a certain namespace and examples of such resources are volume or persistent volume and note.So basically when you create the volume?It’s going to be accessible throughout the whole cluster because it’s not in a namespace and you can actually least components.They are not bound to a namespace using a command cube CTL API resources.Dash dash namespaced false and the same way you can also leased all the resources that are bound to a namespace using namespace true.</p>
<p>说话人 1 01:57:29<br>So now that you’ve learned what the namespaces are why to use them.In which cases it makes sense to use them in which way and also some characteristics that you should consider let’s actually see how to create components in a namespace in the last example.</p>
<p>说话人 1 01:57:46<br>We’ve created components using configuration files.In know where there we have defined in namespace, So what happens is by default.If you don’t provide an namespace to a component.It creates them in a default namespace, so if I apply this config map component.And let’s do that actually right now, so keep CD apply minus F config map.If I apply that an eye.Dooku city will get config map my config map was created in a default namespace and notice that.Even in the Cube Cityel gets config map command.I didn’t use a namespace because Coop City, will get or keep serial commands.They take the default namespace is a default so coupes, it’ll get config map.Is actually same as cube city will get config map dash an or namespace and default namespace so these are the same commands is just a shortcut because it takes default as a default namespace, OK, so one way that I can create this config map.</p>
<p>说话人 1 01:58:45<br>In specific namespace is using cubes it’ll apply command, but adding Flake Namespace.And the namespace name.So this will create config map in my name space and this is one way to do it.</p>
<p>说话人 1 01:59:01<br>Another way is inside the configuration file itself, so I can adjust these config map configuration file.To include the information about the destination namespace itself, so in the metadata.I can add namespace attribute so if I apply this configuration file again using cube CTL apply and now if I want to get the component that I created in this specific namespace.Then I have to add the option or the flag to coosa tailgate comment because as I said, We’re default.It will check only in the default namespace.So I recommend using the namespace attribute in a configuration file instead of providing it to the Cube City L command because one, it’s it’s better documented so you know.By just looking at the configuration file where the component is getting created because there could be an important information and second if you’re using automated deployment where you’re just applying the configuration files.</p>
<p>说话人 1 02:00:03<br>Then again, this will be more convenient way to do it no if for example, we take a scenario where one team gets their own namespace in there has to work entirely in the namespace.It could be pretty annoying to have to.At this namespace tagged every cube cityel command, so in order to make it more convenient.There is a way to change.This default or active namespace, which is default namespace to whatever namespace you choose.And communities or keep still doesn’t have any out of the box solution for that.But there’s a tool called Cube and S or Cubans and you have to install the tool so on Mac.So I’m going to execute brew install.Keep City X so this will install Cubans tool as well, so once I have the Cubans installed I can just.</p>
<p>说话人 1 02:00:58<br>Execute Cubans comment.And this will give me a list of all the namespaces and highlight the one that is active, which is default right now and if I want to change the active namespace, I can do cube ends.</p>
<p>说话人 1 02:01:13<br>My name space.And this will switch the active namespace, so if I do Cubans now.I see that active.One is my name space so now I can execute cubes detail commands without providing my namespace namespace.But obviously if you switch a lot between the namespaces.This will not be so much convenient for your own operating system and environment there will be.</p>
<p>说话人 1 02:01:45<br>Different installation process so I’m going to link the Cube City X installation guide in the description below.So, in this video we’re going to talk about what Ingress is and how you should use it and also what are different use cases for Ingress?So first of all let’s imagine a simple Kubernetes cluster.We have a pot of my application and its corresponding service.My service so the first thing you need for UI application is to be accessible.Through browser right so for external requests to be able to reach your application.So one way to do that and easy way is through an external service, where basically you can access the application.Using HTTP protocol the IP address of the node and the port.</p>
<p>说话人 1 02:02:37<br>However, this is good for test cases.And if you want to try something very fast, but this is not what the?Final product should look like the final product should be like this so you have a domain name for application and you want to secure connection using HTTPS so the way to do that is using this component called?Ingress so you’ll have my app Ingress and instead of external service, you would instead have an internal service.So you would not open your application through the IP address and the.Port and now if the request comes from the browser.It’s going to first reach the Ingress and Ingress then will re directed to the internal service and then it will eventually end up with the pot.</p>
<p>说话人 1 02:03:23<br>So now let’s actually take a look.And see how external service configuration looks like so that you have a practical understanding.So you have the service, which is of type load balancer.This means we’re opening it to public by assigning.An external IP address with the service and this is the port number that user can access the application at so basically IP address the external IP address and the port number that you.Specify here now with Ingress of course, it looks differently.So let’s go through the syntax of Ingress.Basically, you have a kind.Ingress instead of a service and in the specification where the whole configuration happens.You have so called rules or routing rules and this basically defines that you domain address or all the requests to that host must be forwarded to an internal service.So this is the host that user will enter in the browser and in English use define a mapping So what happens when the request to the host gets issued you redirected internally to a service.</p>
<p>说话人 1 02:04:34<br>Path you basically means the URL path so everything after the domain name.So slash whatever path comes up to that you can define those rules here and we’ll see some different examples of the path configuration later and as you see here in this configuration we have HTTP protocol.So later in this video I’m going to show you how to configure HTTPS connection using English component so right now in the specification.We don’t have anything configured for the secure connection is just a.Issue the P Ain one thing to note here is that this HTP attribute here does not correspond to this one here.This is a protocol that the incoming request gets forwarded to to the internal service.So this is actually the second step and not to confuse it with this one.And uh let’s see how the internal service to that Ingress will look like so basically back end is the target where the request.The incoming request will be redirected and the service name should correspond to the internal service name like this.And the port should be internal service port and as you see here.The only difference between the external and internal services is that here in internal service.I don’t have the third port switches.The node ports starting from 30,000, we now have that attribute here.And the type is a default type, not a load balancer, but internal service type, which is cluster.</p>
<p>说话人 1 02:06:06<br>Ip so this should be a valid domain address so you can just write anything here.It has to be first of all valid.And you should map that domain name to IP address of the node that represents an entry point to your Kubernetes cluster so for example, if you decide that one of the nodes inside the cabinets cluster.Is going to be the entry point then you should map this to the IP address of that note or and we will see that later if you configure a server outside of the cabinets cluster that will become the entry points.</p>
<p>说话人 1 02:06:41<br>2 companies cluster then you should map this host name to the IP address of that server.So now that we saw?What Kubernetes Ingress components looks like let’s see how to actually configure Ingress.In the cluster so remember this diagram.I showed you the beginning.</p>
<p>说话人 1 02:06:59<br>So basically you have a pod service in corresponding Ingress now.If you create that English components alone.That won’t be enough for Ingress routing rules to work what you need in addition, is an implementation for Ingress and that implementation is called Ingress Controller, so the step.One will be to install an?Ingress controllers which is basically another pod, or another set of pods that run on your note in your cabinets cluster and us evaluation and processing of Ingress rules.So the yellow file that I showed you with the Ingress component is basically this part right here.And this has to be Additionally installed incumbents cluster So what is Ingress controller?Exactly the function of Ingress Controller is to evaluate all the rules that you have defined in your cluster and this way to manage all the re directions.So basically this will be the entry point.In the cluster for all the requests to that domain or sub domain rules that you’ve configured and this will evaluate all the rules because you may have 50 rules or 50 Ingress components created in your cluster.It will evaluate all.Rules and decide based on that which forwarding rule applies for that specific request so in order to install this implementation of Ingress in your cluster you have to decide which one.Of many different third party implementations.You want to choose from.</p>
<p>说话人 1 02:08:37<br>I will put a link of the whole list and description where you see different kinds of English controllers you can choose from.There is one from Kubernetes itself, which is coordinated.Engine X Inggris Controller, but there are others as well so once we install Ingress Controller in your cluster.You’re good to go create English roles and the whole configuration is going to work so now that I’ve shown you.</p>
<p>说话人 1 02:09:01<br>How Ingress can be used in acuminatus cluster but there is one thing that I think is important to understand in terms of setting up the whole cluster to be able to receive external requests now first of all.You have to consider the environment, where you come in at this cluster is running if you are using some cloud service provider like Amazon Web Services, Google Cloud Linode, there a couple more that have.Out of the book screen.It is solutions or they have their own virtualized load balances etc.Your cluster configuration would look something like this so you would have a cloud load balancer that is specifically implemented by that cloud.Provider and external requests coming from the browser will first heat the load balancer and that load balancer.Then will redirect the requests to English controller.</p>
<p>说话人 1 02:09:57<br>Now, this is not the only way to do it even in cloud environment.You can do it in a couple of different ways, but this is one of the most common strategies.An advantage of using cloud provider for that is that you don’t have to implement a load balancer.Yourself so with minimal effort probably on most cloud providers.You will have the load balancer up and running and ready to receive those requests and forward those requests into your cabinets cluster.So very easy set up now.If you are deploying equipment is cluster on a bare metal environment.Then you would have to do.That part yourself.So basically you would have to configure some kind of entry point to your covenants cluster yourself.And there’s a whole list of different ways to do that.And I’m going to put that also in the description, but generally speaking, either inside of a cluster or outside is a separate server.You will have to provide an entry point in one of those types is an external proxy server, which can be a software, or hardware solution that will take a roll of that.</p>
<p>说话人 1 02:11:10<br>Load balancer an entry point to your cluster so basically what this would mean is that you will have a separate server, and you would give this a public IP address and you would open the ports in order for the requests.To be accepted, and this proxy server, then will act as an entry point to your cluster so this will be the only one accessible externally so none of the servers in your communities cluster will have.Publicly, accessible IP address, which is obviously a very good security practice.So all the requests will enter the proxy server, and that will then redirect the requests to Ingress Controller and English controller.Will then decide which Ingress rule applies to that specific request and the whole internal?Request forwarding will happen.</p>
<p>说话人 1 02:12:01<br>So is it said there are different ways to configure that and to set it up depending on which environment you are, and also which approach you choose.But I think it’s a very important concept to understand how the whole.Cluster setup works so in my case since I’m using mini cube to demonstrate all this on my laptop.The setup will be pretty easy and even though this might not apply exactly to your cluster setting.Still, you will see in practice how all these things work.So the first thing is to install Ingress Controller in Mini Cube and the way to do that is by executing mini cube add ONS enabled.Ingress.So what this does is automatically configures or automatically starts the Kubernetes engine.X implementation of Ingress controller, so that’s 1 of the many third party implementations, which you can also.Safely use in production environments, not just mini cube, but this is what mini cube actually offers you out of the box so with one simple command, Ingress Controller will be configured in your cluster and.If you do Coop City.I’ll get pod in a cube system namespace.You will see the engine X Ingress Controller Pod running in your cluster so once I have English controller installed now I can create an Ingress rule that the controller can.Evaluate so let’s actually head over to the command line, where I’m going to create Ingress rule for Kubernetes dashboard components so in my mini Kube cluster, I have.Kubernetes dashboard, which is right now not accessible externally So what I’m going to do is since I already have internal service for communities dashboard and a pot for that.</p>
<p>说话人 1 02:13:48<br>I’m going to configure an Ingress rule.For the dashboard so I can access it from a browser using some domaining.So I’m going to.So this shows me all the components that I have in common is dashboard.And since I already have internal service.</p>
<p>说话人 1 02:14:07<br>Kubernetes dashboard and the pod.That’s running I can now create an Ingress.Our rule in order to access the Kubernetes dashboard using some host name, so let’s go ahead and do that.So I’m going to create an Ingress for quantities dashboard so these are just metadata.The name is going to be dashboard.Ingress and the namespace.It’s going to be in the same namespace as the service an pot.So, in the specification we’re going to define the rules so the first rule is the host name.I’m just going to call.I’m going to definedashboard.Com.And the HTTP forwarding to internal service.Path let’s leave it at all path.And this is the back end of the service so service name will be what we saw here.So this is the service name.And service port.Is what the service listens so this is actually 80 right here?And this will be it.That’s the English configuration for forwarding every request.That is directed to dashboard.Com to internal carbonated dashboard service and we know it’s internal because its type.These cluster IP so no external IP address so obviously I just made.Host namedashboard.Com it’s not registered anywhere, and I also didn’t configure anywhere, which IP address.This host name should resolve to and this is something that you will.Always have to configure so first of all let’s actually create that Ingress rule so cube city will apply.And it’s called Dashboard Ingress.Mo.See English was created so if I do get increase in the namespace.I should see my English here and as you see address is now empty because it takes a little bit of time to assign the address to Ingress so.We’ll have to wait for that to get the IP address that will map to this host so I’m just going to.Watch this and it’s I see that address was assigned So what I’m going to do now is that I’m going to take that address.An in my.It is see hosts file.</p>
<p>说话人 1 02:16:55<br>At the end I’m going to define that mapping so that IP address will be mapped to dashboard.Dot com and again this works locally if I’m going to typedashboard.Com in the browser.This will be the IP address that it’s going to be mapped to.Which basically means that the request will come into my mini kube cluster will be handed over to English controller and English.Controller then we’ll go and evaluate this rule that I’ve defined here and forward that.</p>
<p>说话人 1 02:17:27<br>Request to service so this is all the configuration, we need so now I’m going to go and.And enterdashboard.Com.And I will see my Kubernetes dashboard here.So Ingress also has something called a default back end, so if I do cube CTL describe Ingress.The name of the Ingress and the namespace.I’ll get this output and here.There is an attribute called default back end.That Maps to default HTTP back end port 80.So what this means is that whenever a request comes into the cabinets cluster.That is not mapped to any back end, so there is no rule for mapping that request to into a service.Then this default back end is used to handle that request.So obviously if you don’t have this service created or defined in your cluster communities will try to forward it to the service.It won’t find it and you would get some default.</p>
<p>说话人 1 02:18:43<br>Error response so for example, if I entered some path that I have configured.I just get page.Not found so it could use it for that is to define custom error messages when.Pages and font when request comes in that you can handle or the application can handle so that users still sees meaningful error message or just the custom page where you can re direct them to your home page.Or something like this so all you have to do is create an internal service with the same name so default HTTP back end and.Port number and also create a pod or application that sends that error, custom error message response.</p>
<p>说话人 1 02:19:29<br>So to know I have shown you what increases and how you can use it.I’ve also shown you a demo of how to create an English rule in Mini Cube.But we’ve used only very basic Ingress Yaml configuration just a simple forwarding to one internal service.With one path, but you can do much more with Ingress configuration than just basic forwarding and in the next section will going to go through more use cases.</p>
<p>说话人 1 02:20:01<br>Of how you can define more fine granular routing for applications inside cabinets cluster so the first thing is defining multiple path of the same host.So consider following use case.Google has one domain, but has many services that it offers so for example, if you have a Google account.You can use its analytics.You can use it shopping.You have a calendar you have a Gmail etc.So all of these are separate applications that are accessible with the same domain so consider, you have an application that does something similar so you offer 2.Separate applications there are part of the same ecosystem, but you still want to have them on separate URLs So what you can do is that in rules, you can define the host.</p>
<p>说话人 1 02:20:53<br>Which is myIP.Com and in the path section you can define multiple path so if user wants to access your analytics application, then they have to enter myapp.Com slash Analytics.And that will forward the request to internal and Analytics Service in the pot or if they want to access the shopping application.Then the URL for that will be myapp.Com slash.Shopping so this way, you can do forwarding with one Ingress of the same host to multiple applications using multiple path another use case is when instead of using?Urls to make different applications accessible some companies use subdomains.So instead of having myapp.Com slash Analytics.They create a sub domain.Analytics dotmyapp.Com, so if you have your application configured that way.Your configuration will look like this.So instead of having one host like in the previous example and multiple path here inside.Now you have multiple hosts where each host represents a sub domain and inside.You just have one path that again redirects that request to analytics service.Pretty straightforward so now in the same request, setting you have analytics service, and apart behind it.Now, the requests will look like this using the sub domain instead of path.</p>
<p>说话人 1 02:22:25<br>And one final topic that I mentioned that will cover here is configuring TLS certificate till now we’ve only seen Ingress configuration for HTTP requests, but it’s super easy to configure.Https forwarding in Ingress, so the only thing that you need to do is define attribute called TLS above the rules section with host which is the same host is right here and.The secret name, which is a reference of a secret that you have to create in a cluster that holds that TLS certificate.So the secret configuration would look like this so the name is the reference right here and the data or the actual contents contain TLS certificate and TLS.</p>
<p>说话人 1 02:23:13<br>Keep if you’ve seen my other videos where I create different components like secret you probably notice that type additional type attribute here in Kubernetes.There is a specific type of a secret.Called TLS so have to use that type when you create a TLS secret and there are 3 small notes To to be made here.One is that the keys of this data have to be named.Exactly like that, the values are the actual file contents of the certificate or key contents and not the file path or location.So you have to put the whole content here base 64 encoded.And the third one is that you have to create the secret in the same namespace as the English component for it to be able to use that otherwise you can’t reference a secret from another namespace.And these 4 lines is all you need to configure mapping of an HTTPS request to that host to internal service.</p>
<p>说话人 1 02:24:22<br>In this video I’m going to explain all the main concepts of helm, so that you are able to use it in your own projects also helmed changes.A lot from version to version so understanding the basic common principles and more importantly, its use cases too.When and why we use helm will make it easier for you to use it in practice, no matter which version you choose.So the topics.I’m going to go through in this video are Helman Helm charts what they are how to use them.And in which scenarios there used and also what is Steeler and what part it plays in the helm architecture So what is helm.</p>
<p>说话人 1 02:24:59<br>Helm has a couple of main features that it’s useful the first one is.Is the package manager for Kubernetes so you can think of it as EPD Yum or homebrew Kubernetes?So it’s a convenient way for packaging collections of Kubernetes.Yaml files and distributing them in public and private registry.</p>
<p>说话人 1 02:25:22<br>Now, this definitions may sound a bit abstract, so let’s break them down.With specific examples.So let’s say you have diploid your application in cabinets cluster.And you want to deploy Elasticsearch.Additionally, new cluster that your application will use to collect its logs.In order to deploy elastic stick in your Kubernetes cluster.You would need a couple of communities components so you would need a stateful set which is for stateful applications like databases.You will need a config map with external configuration, you would need a secret where some credentials.And secret data are stored you will need to create the communities user with its respective permissions, and also create a couple of services.</p>
<p>说话人 1 02:26:12<br>Now, if you were to create all of these files manually.By searching for each one of them separately on Internet be a tedious job and until we have all these yellow files collected and tested and try it out, it might take some time and seems elastic stack deployment.Is pretty much the standard across all clusters other people will probably have to go through the same?So it made perfect sense.Is that someone created this yellow files once and package them up and made it available somewhere so that other people also use the same kind of deployment could use them in their community.Cluster and that bundle of yellow files is called Helm chart.So using helm, you can create your own helm charts or bundles of those yellow files and push them to some helm repository to make it available for others.Or you can consume so you can use download and use existing helm charts that other people pushed and made available in different repositories, so commonly used deployments like database applications.Elasticsearch mongo DB my SQL or monitoring applications like Prometheus that all have this kind of complex setup.All have charts available in some helm repository so using a simple helm install chart name command.You can reuse the configuration that someone else has already made.Without additional effort and sometimes that someone is even the company that created the application.And this functionality of sharing charts that became pretty widely used actually was one of the contributors to why.Home became so popular compared to its alternative tools.</p>
<p>说话人 1 02:28:06<br>So now if your if you have a cluster and you need some kind of deployment that you think should be available out there, you can actually look it up.Either using command line so you can do helm search with a keyword or you can go to either Helms on public repository helm hub or on helm charts pages or other repositories.That are available and I will put all the relevant links for this video in the description.So you can check them out now.</p>
<p>说话人 1 02:28:37<br>Apart from those public registries for helm charts.There are also private registries because.When companies start creating those charts.They also started distributing them among or internally in the organization.So it made perfect sense to create registries to share those charts.Within the organization and not publicly so there are a couple of tools out there.There used as helm charts.</p>
<p>说话人 1 02:29:05<br>Private repositories as well.In other functionality of helm is.That, it’s a templating engine So what does that actually mean imagine you have an application that is made up of multiple microservices and you’re deploying all of them in your cabinets cluster.And deployment and service of each of those microservices are pretty much the same with the only difference that the application name and version are different or the Docker image name and version.Tags are different so without helm.You would write separate YAML files configuration files for each of those microservices so you would have multiple deployment service files where?Each one has its own.Application name and version defined but since the only difference between those yellow files are just a couple of lines or couple of values using helm, where you can do is that you can define.A common blueprint for all the microservices in the values that are dynamic or the values that are going to change.</p>
<p>说话人 1 02:30:14<br>Replace by placeholders and that would be a template file.So the template file would look something like this, you would have a template file which is standard eml, but instead of values in some places, you would have this syntax, which means that you are taking a value from.External configuration and that external configuration if you see the syntax here.</p>
<p>说话人 1 02:30:37<br>Dot values that external configuration comes from an additional yellow file which is called value stored Yemen and.Here you can define all those values that you’re going to use in that template file so for example, here.Those 4 values are defined in and values Yaml file and what values is.It’s an object that is being created.Based on the values that are supplied via values.Yaml file and also through command line.Using dash dash set flag zero, which every way you define those additional values their combined.And put together in dark values object that you can then use in those template files to get the values out.</p>
<p>说话人 1 02:31:25<br>So now instead of having yellow files for each microservice you just have one and you can simply replace those values.Dynamically and this is especially practical when you’re using continuous delivery continuous integration for application because what you can do is that in your builds pipeline, you can use those template demo.</p>
<p>说话人 1 02:31:47<br>Files in replace the values on the fly before deploying them another use case where you can use the helm features of package manager and templating engine.Is when you deploy the same set of applications across different Kubernetes clusters so consider use case?Where you have your microservice application that you want to deploy on development.Staging and production clusters, so instead of deploying the individual yellow files.Separately, in each cluster.You can package them up to make your own application chart that will have.All the necessary yellow files that that particular deployment needs and then you can use them to redeploy the same application in different communities cluster environments.Using one command, which can also make the whole deployment process easier.</p>
<p>说话人 1 02:32:46<br>So now that you know what helm charts are used for it.Let’s actually look at an example helm chart structure to have a better understanding.So typically chart is made up of such a directory structure, so we have the top level will be the name of the chart an inside the directory.You would have following so chart dot UML is basically.The file that contains all the meta information about the chart could be name and version, maybe list of Dependencies etc.Values to demo that I mentioned before, is place where all the values are configured for the template files and this will actually be the default values that you can.</p>
<p>说话人 1 02:33:30<br>Override later.The charts directory will have chart dependencies inside, meaning that if this chart depends on other charts, then those chart dependencies will be stored here and templates.Folder is basically when the template files are stored so when you execute helm install command to actually deploy those yellow files into communities.The template files from here will be.It filled with the values from values dot yamil producing valid communities manifest that can then be diploid into communities.And optionally you can have some other files in this folder like Readme or license file etc.So to have a better understanding of how values are injected into helm templates.</p>
<p>说话人 1 02:34:23<br>Consider that in values that yellow, which is a default value configuration.You have following 3 values image name.Port inversion and as I mentioned the default values that are defined here.</p>
<p>说话人 1 02:34:37<br>Can be over ridden in a couple of different ways one way is that when executing helm install command you can provide an alternative E values Yaml file using values flake.So for example, if values, Yaml file will have following 3 values, which are image name.</p>
<p>说话人 1 02:34:57<br>Porten version you can define your own values, yellow file called my values dot eml an you can.Override one of those values or you can even add some new attributes there and those 2 will be merged which will result into adult values object that will look like this so would have.Image name import from values or demo and the one that you overrode with your own values file.</p>
<p>说话人 1 02:35:25<br>Alternatively, you can also provide additional individual values using?Set flag where you can define the values directly on the command line but of course.It’s more organized and better manageable to have files where you store all those values instead of just providing them on the command line.</p>
<p>说话人 1 02:35:45<br>Another feature of helm is release management, which is provided based on it’s set up but it’s important to note here.The difference between health versions 2 and 3 in version 2.</p>
<p>说话人 1 02:35:59<br>Of helm to helm installation comes in 2 parts.You have helped client and the server and the server part is called Tiller so whenever you deploy helm chart using helm install.My chart help client will send the yellow files to Taylor that actually runs has to run in a Kubernetes cluster and Taylor then will execute this request and create components.From this yellow files inside the currents cluster and exactly this architecture offers additional valuable feature of helm, which is release management so the way helm clients server setup works.Is that whenever you create or change deployement?Peeler will store a copy of each configuration client sent for future reference thus creating a history of chart executions.So when you execute helm upgrade chart name.The changes will be applied to the existing deployment instead of removing it and creating a new one and also in case the upgrades goes wrong for example, some yellow files where false or some configuration was wrong.You can rollback that upgrade using helm rollback chart name comment.And all this is possible because of that chart execution history that Taylor keeps whenever you send those requests from helm client to dealer.</p>
<p>说话人 1 02:37:29<br>However, this setup has a big caveat, which is.That Taylor has too much power inside the covenants, cluster, it can create update delete compose.And it has too much permissions.And this makes it actually a big security issue and this was one of the reasons why in hell 3.They actually removed.The teal apart and it’s just the simple helm.Binary now and it’s important to mention here.Because a lot of people have heard of Taylor and when you deploy helm version 3.You should be confused that Taylor isn’t actually there anymore.In this video I will show you how you can persist data in Kubernetes using volumes.We will cover 3 components of Kubernetes Storage.Persistent volume persistent volume claim and storage class.</p>
<p>说话人 1 02:38:24<br>And see what each component does and how it’s created and used for data persistence consider a case where you have a my SQL database pod, which your application uses data gets added.Updated in the database, maybe you create a new database with a new user etc.But default when you restart the pod.All those changes will be gone because Kubernetes doesn’t give you.Data persistence out of the box that something that you have to explicitly configure for each application that needs saving data between pod restarts.So basically you need a storage that doesn’t depend on the Pod Lifecycle.So it will still be there.When Paradise in new one gets created so the new part can pick up where the previous one left.Off so it will read the existing data from that storage to get up-to-date data.However, you don’t know on which node the new pod restarts.So, your storage must also be available on all nodes.</p>
<p>说话人 1 02:39:29<br>Not just one specific one so that when the new pod tries to read the existing data.The up-to-date data is there on any node in the cluster.Anne also you need a highly available storage that will survive, even if the whole cluster crashed.So these are the criteria or the requirements that your storage for example, your database storage will need to have to be reliable.</p>
<p>说话人 1 02:39:59<br>Another use case for persistent storage, which is not for database.Is a directory?Maybe you have an application that writes and reads files from pre configured directory.This could be session files for application or configuration files etc.An you can configure.</p>
<p>说话人 1 02:40:16<br>Any of these type of storage using carbonated component called persistent volume.Think of a persistent volume as a cluster resource just like ram or CPU that is used to store.</p>
<p>说话人 1 02:40:30<br>Data.Persistent volume just like any other component gets created using communities yellow file where you can specify the kind which is persistent volume and in the specification section you have to define.Different parameters like how much storage should be created for the volume.But since persistent volume is just an abstract component.It must take the storage from the actual physical storage right.Like local hard drive from the cluster nodes or your external NFS servers outside of the cluster or maybe cloud storage like KWS block storage or from Google Cloud Storage.Etc.So the question is where does this storage back end come from local or remote sore on cloud.Who configures it?Who makes it available to the cluster and that’s the tricky part of data persistence in communities because Kubernetes doesn’t care about your actual storage it gives you?</p>
<p>说话人 1 02:41:33<br>Persistent volume component as an interface to the actual storage that you as a maintainer or administrator have to take care of so you have to decide what type of storage your cluster.Services or applications would need and create an manage them by yourself managing, meaning do backups and make sure they don’t get corrupt etc.</p>
<p>说话人 1 02:41:57<br>So think of storage in Kubernetes.As an external plug into your cluster whether it’s a local storage on actual nodes where the cluster is running or a remote storage doesn’t matter there all plugins to the cluster.And you can have multiple storage is configured for your cluster where one application in your cluster uses local disk storage.Another one uses the NFS server and another one uses some cloud storage.Or one application may also use multiple of those storage types and by creating persistent volumes.</p>
<p>说话人 1 02:42:36<br>You can use this actual physical storage is so in the persistent volume specification section.You can define which storage back end, you want to use to create that storage abstraction or storage resource for applications, so this is an example where we use.</p>
<p>说话人 1 02:42:55<br>Nfs storage back end, so basically we define how much storage.We need some additional parameters to that storage like should it be readwrite or readonly, etc and the storage back end.With its parameters.And this is another example where we use, Google Cloud as a storage back end again with the storage back end specified here and capacity and access modes here now, obviously depending on the storage type.On the storage, Beck and some of the attributes in the specification will be different because there are specific to the storage type.</p>
<p>说话人 1 02:43:35<br>This is another example of a local storage, which is on the node itself.Which has additional node affinity attribute now you don’t have to remember and know all these attributes at once because you may may not need all of them and also I will make separate videos covering some of the most used volumes.And explain them individually with examples and demos so there.</p>
<p>说话人 1 02:44:00<br>I’m going to explain in more detail, which attributes should be used for this specific volumes and what they actually mean in the official Kubernetes documentation, you can actually see the complete list.Of more than 25 storage backends that Kubernetes supports note here that persistent volumes are not namespaced meaning there accessible to the whole cluster.In unlike other components that we saw like parts and services.They’re not in any namespace there just available to the whole cluster to all the namespaces.</p>
<p>说话人 1 02:44:35<br>Now it’s important to differentiate here between 2 categories of the volumes local and remote each volume type in these 2 categories has its own use case.Otherwise, they won’t exist.And we will see some of these use cases later in this video.However, the local volume types violate the second and third requirements of data persistence for data.This is that I mentioned at the beginning, which is one, not being tide to one specific node.But rather to each node equally because you don’t know where the new pod will.Start in the second surviving in cluster crash scenarios because of these reasons for database persistence.You should almost always use remote storage.So who creates these persistent volumes and win.As I said persistent volumes are resources like CPU or ram so they have to be already there.In the cluster when the pod that depends on it or that uses it is created so a side note here.That there are 2 main roles in Kubernetes.There is an administrator who sets up the cluster and maintains it and also make sure the cluster has enough resources.These are usually system administrators or DevOps engineers and accompany.In the second role is communities user that deploys the applications in the cluster either directly or through CI pipeline.These are developer.Devops teams who create the applications and deploy them.In this case, the Kubernetes administrator would be the one to configure the actual storage, meaning to make sure that the NFS server storage?Is there an configured or maybe create and configure.A cloud storage that will be available for the cluster and second create persistent volume components.From this storage beckons based on the information from developer team of what types of storage.Their applications would need and the developers, then we’ll know that storage is there an can be used by their applications.But for that developers have to explicitly configure the application.Yellow file to use those persistent volume components in other words, application has to claim that volume storage and you do that using another component of Kubernetes called persistent.</p>
<p>说话人 1 02:47:09<br>Volume claim persistent volume claims also PVCS are also created with Yaml configuration.Here’s an example claim again don’t worry about understanding each and every attribute that is defined here.Put on the higher level, the way it works is that PVC claims of volume with certain storage size or capacity, which is defined in the persistent volume claim an some additional characteristics.Like excess type should be read only or read rights or the type etc.And whatever persistent volume matches.This criteria or in other words, satisfies this claim.</p>
<p>说话人 1 02:47:51<br>Will be used for the application, but that’s not all, you have to now use that claim in your parts configuration like this so in the pod specification here, you have the volumes.Attribute that references the persistent volume claim with its name.So now the pod and all the containers inside the pod will have access to that.Persistent volume storage so to go through those levels of abstractions that by step pods excess storage by using the claim, as a volume right so they request, the volume?Through claim the claim, then we’ll go and try to find a volume persistent volume in the cluster that satisfies the clay.In the volume will have a storage, the actual storage back end, that it will create that storage resource from in this way.The pod will now be able to use that actual.</p>
<p>说话人 1 02:48:54<br>Storage back end note here that claims must exist in the same namespace as the pod using the claim while as I mentioned before persistent volumes are not namespaced so once the pod.Finds the matching persistent volume through the volume claim through the persistent volume claim.The volume is then mounted into the pod like this here.This is a pod level.And then that volume can be mounted into the container inside the pot, which is this level right here.And if you have multiple containers here in the pod you can decide.To Mount this volume in all the containers or just some of those so now the container.And the application inside the container can read and write to that storage and when the pod dies in new one gets created it will have access to the same storage and see all the changes the previous pod, or the previous containers.</p>
<p>说话人 1 02:49:54<br>Meet again the attributes here like volumes and volume mounts, etc and how they’re used.I will show you more specifically and explain in a later demo video now you may be wondering why so many abstractions.For using volume where admin role has to create persistent volume and reuse.The role creates a claim on that.Persistent volume and that is in use in pot?Can I just use one component and configure everything there?</p>
<p>说话人 1 02:50:21<br>Well, this actually has a benefit because as a user meaning a developer who just wants to deploy their application in the cluster.You don’t care about where the actual storage is you know you want your database to have persistence.And whether the data will leave on the gluster FS or awos or local storage doesn’t matter for you as long as the data is safely stored or if you need a directory storage.For files you don’t care where the directory actually leaves as long as it has enough space and works properly and you sure don’t want to care about setting up these actual storage is yourself, you just want.50 Gigabyte storage for your elastic or 10 Gigabyte for application.That’s it.So you make a claim for storage using PVC and assume that cluster has storage resources.Already, there, and this makes diploid the applications easier for developers because they don’t have to take care of the stuff beyond deploying the applications.</p>
<p>说话人 1 02:51:28<br>Now there are 2 of volume types that I think needs to be mentioned separately because they are a bit different from the rest and these are config map and secret now.If you have watched my other video on components components.Then you are already familiar with both both of them are local volumes.But unlike the rest.These 2 aren’t created via PV and PVC but a rather own components and managed by Kubernetes itself.</p>
<p>说话人 1 02:51:57<br>Consider a case where you need a configuration file for your Prometheus pod.Or maybe a message broker service like mosquito or consider when you need a certificate file mounted inside your application.In both cases, you need a file available to your pod, so how this works is that you create config map or secret component.You can Mount that into your pod and into your container the same way as you would Mount persistent volume claim.So instead you would have a conflict map or secret here and I will show you a demo of this in a video where I cover local volume types.</p>
<p>说话人 1 02:52:37<br>So to quickly summarize what we’ve covered so far.You see at its core volume is just a directory, possibly with some data in it, which is accessible to the containers in a pod how that directory is made available or what storage medium actually.Effects that and the contents of that directory are defined by specific volume type you use?</p>
<p>说话人 1 02:53:03<br>To use in volume apart specifies what volumes to provide for the pod in the specification volumes attribute and inside the pool then you can decide where to Mount.That storage into using volume mounts attribute inside the container section.And this is a path inside the container where application can access whatever storage.We mounted into the container and as I said if you have multiple containers, you can decide.Which container should get access to that storage interesting note for you?Is that a pod can actually use multiple volumes of different types simultaneously.</p>
<p>说话人 1 02:53:47<br>Let’s say you have an elastic search.Application or pod running in your cluster that needs a configuration file mounted through config map needs a certificate.Let’s say client certificate amounted.As a secret Anet needs database storage.Let’s say which is backed with AWS elastic block storage so in this case you can configure all 3 inside your pot.Or Deployement.So this is the part specification that we saw before.And here on the volumes level, you will just list all the volumes that you want to Mount into your pod.So let’s say you have a persistent volume claim that in the background claims.Persistent volume from AWS block storage.</p>
<p>说话人 1 02:54:38<br>And here you have the config map and here have a secret and here in the volume mounts you can list.All those storage mounts using the names right so you have the persistent storage.Then you have the config map and secret and each one of them is mounted to a certain path inside the container.</p>
<p>说话人 1 02:55:00<br>Now we saw that to persist data in communities admins need to configure storage for the cluster create persistent volumes and developers then can claim them, using PC’s but consider cluster with hundreds of applications.Where things get deployed daily and storage is needed for these applications so developers need to ask admins to create persistent volumes.They need for applications before deploying them and admins, then may have to manually request storage from cloud or storage provider.And create hundreds of persistent volumes for all the applications that need storage manually and that can be tedious time consuming and can get messy very quickly.So to make this process more efficient.</p>
<p>说话人 1 02:55:48<br>There is a third component of Kubernetes persistence called storage class storage class basically creates or provisions persistent volumes.Dynamically.Never PVC claims it and this way, creating or provisioning volumes in a cluster.Maybe automated storage class also gets created using Yaml configuration file so this is an example file.We have the kind storage class storage class creates persistent volumes dynamically in the background.So remember, we defined storage back end in the persistent volume component.Now we have to define it.In the storage class component and we do that using the provisioner attribute.</p>
<p>说话人 1 02:56:34<br>Which is the main part of the storage class configuration because it tells Kubernetes which provision are to be used for a specific storage platform or cloud provider to create the persistent.Volume component out of it, so each storage back end has its own provisioner that communities offers internally, which are prefixed with communities dot IO like this one here.</p>
<p>说话人 1 02:57:00<br>And these are internal provisioners and for others or other storage types their external provisioners that you have to then explicitly going find and use that?In your storage class.And in addition to provision or attribute we configure parameters of the storage.We want to request for our persistent volume like this ones here, so storage classes basically another abstraction level.That abstracts the underlying storage provider as well as parameters for that storage characteristics for the storage like what disk type or etc.So how does it work or how do you use?</p>
<p>说话人 1 02:57:41<br>Storage class in the pod configuration same as persistent volume.It is requested or claimed by PVC so in the PVC configuration here we add additional attribute.That is cold storage class name that references the storage class to be used to create a persistent volume that satisfies the claims.Of this PC so now when a pod claims storage through PVC.The PVC will request that storage from storage class, which then will provision or create.Persistent volume that meets the needs of that claim using provisioner from the actual storage back end now.</p>
<p>说话人 1 02:58:29<br>This should help you understand the concepts of how data is persisted in communities.Is a high level overview?In this video we’re going to talk about what stateful settings.In Kubernetes, an what purpose.It has So what is stateful set its accumulated component that is used specifically for stateful applications?So, in order to understand that first you need to understand what a stateful application is examples of stateful applications are all databases like my SQL Elasticsearch mongo DB etc.Or any application that stores data to keep track of its state in other words.These are applications that tracks state by saving that information in some storage stateless applications on the other hand.Do not keep records of previous interaction in each request or interaction is handled as a completely new isolated interaction based entirely on the information that comes with it.And sometimes stateless applications connect to the stateful application to forward those requests.</p>
<p>说话人 1 02:59:40<br>So imagine a simple setup of a node JS application that is connected to Mongo.Db database when a request comes into the node JS application.It doesn’t depend on any previous data to handle this incoming.Request it can handle it based on the payload in the request itself.Now a typical such request will Additionally need to update some data in the database or query.The data that’s where mongo DB comes in.So when node JS forwards that request to Morgan to be.Mongo DB will update the data based on its previous state or query.The data from its storage so for each request.It needs to handle data and obviously always depends on the most up to date data.Or state to be available while node JS is just a pass through for data updates or queries and it just processes code.</p>
<p>说话人 1 03:00:37<br>Now, because of this difference between stateful and stateless applications.There both diploid different ways, using different components in Kubernetes stateless applications.Are diploid using deployment component but deployment?Is an abstraction of parts an allows you to replicate that application meaning run 2510 identical parts of the same stateless application.In the cluster so while stateless applications are diploid using deployement stateful applications in Kubernetes are diploid using stateful set components in just like deployments stateful set makes it possible to replicate this stateful.Yep, parts or to run multiple replicas of it in other words, they both manage parts that are based on an identical container specification.And you can also configure storage with both of them equally in the same way so if both manage the replication of pots and also configuration of data persistence in the same way.</p>
<p>说话人 1 03:01:47<br>The question is what a lot of people ask in are also often confused about what is the difference between those 2 components?Why we use different ones for each type of application so in the next section.We’re going to talk about that.Difference is now replicating stateful application is more difficult and has a couple of requirements that stateless.</p>
<p>说话人 1 03:02:09<br>Applications do not have so let’s look at this first with the example of my SQL database.Let’s say you have one my SQL database pod that handles requests from a Java application, which is deployed using a deployment component and let’s say you scale the Java application to 3 parts.So they can handle more client requests in parallel.You want to scale my SQL app.So we can handle more Java requests as well, scaling your Java application here is pretty straightforward Java.Applications replica parts will be identical and interchangeable so you can scale it.</p>
<p>说话人 1 03:02:49<br>Using the deployment pretty easily deployment will create the pods in any order in any random order.They will get random hashes at the end of the pod name, they will get one service that load balances to any one of the replica parts.For any requests and also when you delete them.They get deleted in a random order or at the same time right or when he scaled them down from 3 to 2 replicas for example, one random replica part gets.Chosen to be deleted so no.Vacations there on the other hand, my SQL pod.Replicas cannot be created and deleted at the same time in any order and they can’t be randomly addressed and the reason for that is.Because the replica parts are not identical.In fact, they each have their own additional identity on top of the common blueprint of the part that they get created from.Giving each part its own required individual identity is actually what stateful set does different from deployment.It maintains a sticky identity for each of its parts.And as I said, these pods are created from the same specification, but they’re not interchangeable.Each has a persistent identifier that it maintains across in a rescheduling so meaning when pod dies.And gets replaced by new part it keeps their identity so the question you may be asking now is why do these parts need their own identities?Why they can’t be interchangeable just like with deployment so why is that?</p>
<p>说话人 1 03:04:28<br>And this is a concept that you need to understand about scaling database applications in general.When you start with a single my SQL pod.It will be used for both reading and writing data.But when you add a second one, it cannot act the same way because if you allow 2 independent instances of my SQL.To change the same data you will end up with data inconsistency so instead.There is a mechanism that decides that only one.Paul is allowed to write or change the data, which is shared.Reading at the same time by multiple parts my SQL instances from the same.Data is completely fine and the part that is allowed to update the data is called the master the others are called slaves.So this is the first thing that differentiates these parts from each other so not all ports are same identical.But there is a must pot and then the slave parts right and there is also difference between those slave parts.</p>
<p>说话人 1 03:05:29<br>Terms of storage, which is the next point, so The thing is that this pods do not have access to the same physical storage.Even though they use the same data.They’re not using the same physical storage.Of the data.They each have their own replicas of this storage that each one of them can access for itself, and this means that each pod replica at anytime must have the same data as the other ones.And in order to achieve that they have to continuously synchronize their data and since Master is the only one allowed to change data and the slaves need to take care of their own data storage.Obviously, the slaves must know about.Each such change, so they can update their own data storage to be up to date for the next query requests and there is a mechanism in such clustered database setup that allows for continuous data.</p>
<p>说话人 1 03:06:26<br>Pronunciation.Master changes data and all slaves update their own data storage to keep In Sync and to make sure that each pod has the same state now let’s say you have one master into slave parts.</p>
<p>说话人 1 03:06:41<br>Of my SQL now, what happens when a new pod replica joins the existing setup because now that new pod also needs to create its own storage and then take care of synchronizing it?</p>
<p>说话人 1 03:06:54<br>What happens is that it first.Close the data from the previous part, not just any part in the in.Set up but always from the previous part in once it has the up-to-date data cloned it starts continuous synchronization as well to listen for any updates by must report and this also means an I want to point this out since it’s pretty interesting.</p>
<p>说话人 1 03:07:15<br>Point.It means that you can actually have a temporary storage for a stateful application and not persist.The data at all since the data gets replicated between the pots, so theoretically.It is possible to just rely on data replication.</p>
<p>说话人 1 03:07:31<br>Between the pots, but this will also mean that the whole data will be lossed when all the pods die, so for example, if stateful set gets deleted or the cluster crashes or all the nodes where these pod replicas are running crash.And every pod dice at the same time, the data will be gone and therefore it’s still a best practice to use data persistence for stateful applications.If losing the data will be unacceptable, which is the case in most database applications.And with persistent storage data will survive, even if all the parts of the stateful set dying or even if you delete the complete stateful set component in all the parts get wiped out as well.The persistent storage and the data will still remain because persistent volume.Lifecycle isn’t connected or isn’t tide to life cycle of other components like?Deployment or stateful set and the way to do this is configuring persistent volumes for your stateful set and since each pod has its own data storage.Meaning it’s the.Own persistent volume that is then picked up by its own physical storage, which includes the synchronized data or the replicated database data but also the state of the pod so each pod.Has its own state, which has information about whether it’s a master port or a slave or other individual characteristics and all of these get stored in the pods own storage.And that means when a pot dies and gets replaced the persistent pot.Identifiers make sure that the storage volume gets reattached to the replacement pot.Is a sad because that storage has the state of the pod?In addition to that replicated data.I mean, you can clone the data again.There will be no problem.But it shouldn’t lose its state or identity state so to say, and for this reattachment to work.It’s important to use a remote storage because if the pod gets rescheduled from OneNote to another node.The previous storage must be available on the other node as well, and you cannot do that using local.</p>
<p>说话人 1 03:09:52<br>Volume storage because they are usually tide to a specific node and the last difference between deployment and stateful set is something that I mentioned before is the pot identifier.Meaning that every pod has its own identifier so unlike deployment, where parts get random hashes at the end stateful set parts get fixed ordered names, which is made up of the stateful set name.In the ordinal it starts from zero and each additional part will get the next numeral.So if you create a stateful set called my SQL with 3 replicas.You’ll have parts with names must equal 01 and 2.The first one, is the Master and then come the slaves in the order of start up an important note here is that?The stateful set will not create the next pod in the replica if the previous one isn’t already up and running if first pod creation for example, failed.Or if it was pending the next one won’t get created at all.It would just wait in the same order is held deletion.But in reverse order so for example, if you delete the stateful set or if you scaled it down too.One for example, from 3, the deletion will start from the last pot.So my SQL tool deleted first it will wait until that part is successfully deleted and then it will delete my SQL.One and then it will delete my SQL zero and again.</p>
<p>说话人 1 03:11:24<br>All these mechanisms are in place in order to protect the data and the state that this stateful application depends on in addition to this.Fixed predictable names each pod in a stateful set gets its own DNS endpoint from a service so there’s a service name for the stateful application just like for deployement for example, that will address any.Replica pot and plus.</p>
<p>说话人 1 03:11:50<br>In addition to that there is individual DNS name for each pod, which deployement pods do not have the individual DNS names are made up of pod name and.The manage of the governing service name, which is basically a service name that you define inside the stateful set so these 2 characteristics, meaning having a predictable or fixed.Name as well as its fixed individual DNS name means that when pod restarts.The IP address will change.But the name in endpoint will stay the same.That’s why it said pods get sticky identities so it.Gets stuck to it, even between the restarts and the sticky identity makes sure that each replica pod can retain its state and its role.Even when it dies and gets recreated and finally I want to mention important point here.Is it see replicating stateful apps like databases with its persistent storage requires a complex mechanism in Kubernetes helps you and supports you to set this whole thing up, but you still need to do.A lot by yourself with Kubernetes doesn’t actually help you or doesn’t provide you out of the box solutions.For example, you need to configure the cloning and data synchronization inside the stateful set and also make the remote storage available.As well as take care of managing and backing it up all of this, you have to do yourself and the reason is that stateful applications are not a perfect candidate for containerized environments.In fact, Docker Kubernetes and generally containerization.Is perfectly fitting for stateless applications that do not have any state data dependency and only process code so scaling and replicating them in containers is super easy.</p>
<p>说话人 1 03:13:46<br>In this video I will give you a complete overview of Kubernetes Services first.I’ll explain shortly.What service component is in Kubernetes and when we need it and then we’ll go through the different service types.Cluster IP service headless service note port and load balancer services.I will explain the differences between them.And when to use which 10 by the end of the video you will have a great understanding.Of community services and will be able to use them in practice, so let’s get started.So.What is the service in Kubernetes in?</p>
<p>说话人 1 03:14:21<br>Why do we need it in Kubernetes cluster each part gets its own internal IP address but the pods in communities are ephemeral, meaning that they come and go very frequently.And when the pod restarts or when old one dies and the new one gets started in its place.It gets a new IP address.So it doesn’t make sense to use pot IP addresses directly because then you would have to adjust that.Every time the pod gets recreated with the service.However, you have a solution of a stable or static IP address that stays even when the pod dies, so basically in front of each pod, we set a service.Which represents a persistent stable IP address excess that pod service also provides load balancing because when you have pod replicas for example, 3 replicas of your micro service application.Or 3 replicas of my SQL application.The service will basically get each request target to that my SQL or your microservice application and then forwarded to one of those.Parts so clients can call a single stable IP address instead of calling each pod individually so services are a good abstraction for loose coupling for communication within the cluster.So within the cluster components or parts inside the cluster, but also from external services like if you have browser requests coming to the cluster or if you’re talking to an external database for example.</p>
<p>说话人 1 03:15:56<br>There are several types of services in Kubernetes, the first and the most common one that you probably will use most of the time is the cluster IP type.This is a default type of service.Meaning when you create a service and not specify type.It will automatically take cluster.Ip is a type so let’s see how cluster IP works an where it’s used in Kubernetes setup.Imagine we have a micro service application deployed in the cluster so we have a pod with microservice container running inside that pod and beside that microservice container we have.A sidecar container that collects the logs of the microservice and then sends that to some destination database.So these 2 containers are running in the pot and let’s say your microservice container is running at pod.3000 and you’re logging container.Let’s say is running on port 9000.This means that those 2 ports will be now open and accessible inside the pot.And pod will also get an IP address from a range that is assigned to a note.So the way it works is that if you have for example, 3 worker nodes in your community.Cluster each worker node will get a range of IP addresses, which are internal in the cluster so for example, the pod.One will get IP addresses from a range of 10.2, .1.Onwards the second worker node will get this IP range and the third worker node will get this one.So let’s say this pod starts on node 2.So it get IP address that looks like this, if you want to see the IP addresses of.Your pods in the cluster you can actually check them using cubes.</p>
<p>说话人 1 03:17:51<br>A tailgate pod.Output wide command where you will get some extra information about the pods, including its IP address in here.See the IP address that it got it signed and as I mentioned these are from the IP address range that each worker node in the cluster will get so this is from the first worker node and these are from the second worker node.So now we can access those containers inside the pot.If this IP address at these ports if we set the replica count to 2.We’re going to have another pod, which is identical to the first one, which will open the same.Ports and it will get a different IP address.Let’s say if it starts on worker node.One you will get an IP address that looks something like this.</p>
<p>说话人 1 03:18:37<br>Now let’s say this microservices accessible through a browser.So we have Ingress configured and the requests coming in from the browser to the micro service will be handled by Ingress?How does this incoming request get forwarded from Ingress?All the way to the pod and that happens through a service.</p>
<p>说话人 1 03:18:59<br>A cluster IP or so-called internal service.A service in Kubernetes is a component just like a pod, but it’s not a process.It’s just an abstraction layer that basically represents an IP address so service will get an IP address that it is iaccessible at and service will also be accessible at a certain port.</p>
<p>说话人 1 03:19:20<br>Let’s say we define that.For it to be 3200 so Ingress will talk to the service or hand over the request to the service at this IP address at this port so this is how service is.Accessible within the cluster so the way it works is that we define Ingress rules that forward the request based on the request address to certain services and we define the service by.Its name and the DNS resolution than Maps.That service name to an IP address that this service actually got assigned so this is how Ingress knows.How to talk to the service so once the request gets handed over to the service at this address and service will know to forward that request to one of those parts that are registered as.</p>
<p>说话人 1 03:20:15<br>The service endpoints now here 2 questions how does ServiceNow which pods it is managing or which parts to forward the request to and the second one is how the service know which port.To forward that request 2 on that specific pod.The first one, is defined by selectors as service identifies its member pots or its endpoint parts using selector attribute zero in the service specification.</p>
<p>说话人 1 03:20:47<br>Yellow file from which we create the service.We specify the selector attribute that has a key value pairs defined as least now.This key value pairs are basically labels that pots should have.To match that selector so in the pod configuration file we assign the pod.Certain labels in the metadata section and this labels can be arbitrary name.So we can say my app for example.And give it some other labels.This is basically something that we define ourselves.We can give it any name that we want.These are just key value pairs that identify a set of pots and in the service animal file then we define a selector.To match any part that has all of these labels.This means if we have a deployment component that creates 3 replicas of pots with label called app my app.And type micro service for example, an in service selector attribute.We define those 2 labels then service will match all of those, 3 pod replicas and it will register all 3 parts.As its endpoints and as I said it should match all the selectors, not just one so this is how service will know which parts belong to it, meaning where to forward that request to.</p>
<p>说话人 1 03:22:07<br>The second question was if a part has multiple ports open with 2 different applications are listening inside the pod.How does ServiceNow?Which ports to forward the request to and this is defined?In the target port attribute so this target port attribute so let’s say target port in our example is 3000.What this means is that when we create the service.It will find all the parts.That match this selector so this ports will become endpoints of the service and when the service gets a request.It will pick one of those pod replicas randomly because it’s a load balancer and it will send the request.Received to that specific pod on a port defined by Target.Port attribute in this case 3000.</p>
<p>说话人 1 03:23:00<br>Also note that when you create a service.Kubernetes creates an endpoints object that has the same name as the service itself.An Kubernetes will use these endpoints object to keep track of which pods are members of the service.Or as I said, which parts are the endpoints of the service and since this is dynamic because whenever you create a new pod replica or a pod dies.The endpoints get updated so this object will basically track that.And note here that the service port itself is arbitrary so you can define it yourself, whereas the target port is not arbitrary.It has to match the port where container the application container inside the pod.Is listening at now let’s say our microservice application got its request from the browser through Ingress.An internal cluster IP service and now it needs to communicate with the database.To handle that request for example, an in our example.Let’s assume that the Micro Service application uses.</p>
<p>说话人 1 03:24:08<br>Mongo DB database.So we have 2 replicas of mongo DB in the cluster which.Also have their own service endpoint so mongo DB service is also of cluster IP and it has its own IP address so now the microservice application inside the pod.Can talk to the mongo DB database also using the service endpoint so the request will come from one of the parts that gets the request from the service to the mongo DB service.At this IP address and the port that service has open and the service will again select one of those pod replicas and forward that request to the selected pod.It’s the port the target port defined here and this is the port where mongo DB application inside the pod is listening at now let’s assume that inside that mongo DB pod.There is another container running that selects the monitoring metrics for Prometheus for example.And that will be a mongo DB Explorer and that container.Let’s say is running at port now.Thousand 216 and this is where the application is accessible at an in the cluster.We have a Prometheus application that scrapes.The metrics endpoint from this mongo DB exporter container.</p>
<p>说话人 1 03:25:34<br>From this end point now that means that service has to handle 2 different endpoint requests, which also means that service has 2 of its own ports open.For handling these 2 different requests one from the clients that want to talk to the mongo.Db database and one from the clients like Prometheus that want to talk to the mongo DB export are application.And this is an example of a multi port Service, an note here.That’s when you have multiple ports defined in a service, you have to name those ports if it’s just one port then you can.Leave it so to say anonymous you don’t have to use the name attribute it’s optional.But if you have multiple ports defined.Then you have to name each one of those so these were examples of cluster IP service type.</p>
<p>说话人 1 03:26:28<br>Now let’s see another service type, which is called headless service.So let’s see what headless service type is as we saw each request to the service is forwarded to one of the pod replicas.That are registered as service endpoints.</p>
<p>说话人 1 03:26:44<br>But imagine if a client wants to communicate with one of the parts directly and selectively or what, if the endpoint pods need to communicate with each other directly.Without going through the service, obviously in this case, it wouldn’t make sense to talk to the service endpoint, which will randomly select one of the pots because we want the communication with specific pots now, what would be such a use case.A use case where this is necessary is when we are deploying stateful applications in Kubernetes.</p>
<p>说话人 1 03:27:18<br>Stateful applications like databases.My SQL Mongo.Db Elasticsearch and so on in such applications that pod replicas.Aren’t identical but rather each one has its individual state and characteristic for example.If we’re deploying a my SQL application.You would have a master instance of my SQL an worker instances.Of my SQL application and master is the only part allowed to write to the database and the worker pods must connect to the Master to synchronize their data after master pod has.Made changes to the database so they get the up-to-date data as well, an when new worker pod starts.It must connect directly to the most recent worker node to clone the data from.Anne also get up to date with the data state so that’s the most common use case where you need direct communication with individual pots for such case for client to connect to all parts.Individually, it needs to figure out the IP address of each individual pod.</p>
<p>说话人 1 03:28:26<br>One option to achieve.This would be to make an API call to coordinate this API server.And it will return the list of parts and their IP addresses.But this will make your application to tide to the Kubernetes specific API and also this will be inefficient because you will have to get.The whole list of parts and their IP addresses every time you want to connect to one of the parts.</p>
<p>说话人 1 03:28:51<br>But as an alternative solution Kubernetes allows clients to discover pot IP addresses through DNS.Look up.Ann usually the way it works is that when a client performs a DNS.Look up for a service.The DNS server returns a single IP address, which belongs to the service and this will be this services cluster IP address.Which we saw previously however if you tell Kubernetes that you don’t need a cluster IP address of the service by setting the cluster.Ip field to none when creating this service, then the DNS server will return the pot IP addresses instead.Of the services IP address another client can do a simple DNS.Look up to get the IP address of the pods that are members of that service and then client can use that IP address to connect to.The specific party wants to talk to or all of the parts, so the way we define a headless service in a service configuration file is basically setting the cluster IP to none.So when we create these service from this configuration file.Bernese will not assign the service at cluster IP address.And we can see that in the output when I list.</p>
<p>说话人 1 03:30:08<br>My services so I have a cluster.Ip service that I created for the microservice.Anna headless service and note here that when we deploy stateful applications.In the cluster like Mongo, DB for example, we have the normal service that cluster.Ip service that basically handles the communication to mongo DB and maybe other container inside the pod.And in addition to that service we have a headless service.So we always have these 2 services alongside each other so this can do the usual load balancing stuff for these kind of use case and.</p>
<p>说话人 1 03:30:47<br>For use cases where client needs to communicate with one of those parts directly like a master node directly to perform the right commands or the parts to talk to each other for data synchronization.The headless service will be used for that.</p>
<p>说话人 1 03:31:04<br>When we define a service configuration, we can specify a type of the service and the type attribute can have 3 different values.It could be cluster IP which is the default.That’s why we don’t have to specify that we have node port and load balancer.So type node.Port basically creates a service that is accessible on aesthetic port on each worker node in the cluster.</p>
<p>说话人 1 03:31:30<br>Not to compare that to our previous example.The cluster IP services only accessible within the cluster itself, so no external traffic can directly address the cluster IP service.The node port service, however, makes the external traffic accessible on static or fixed port on each worker node so in this case instead of Ingress, the browser requests will come directly.</p>
<p>说话人 1 03:31:59<br>To the worker node at the port that the service specification defines in the port that note port service type exposes is defined in the node port attribute.And here note that the note port value has a predefined range between 30,032 thousand 767.So you can have one of the values from that range.As a note port value anything outside that range won’t be accepted so this means that the node.</p>
<p>说话人 1 03:32:33<br>Port services accessible for the external traffic like browser requests for example, add IP address of the work.Note and the note port defined here, however, just like in cluster IP.We have a port of the service.So when we create the node port service.A cluster IP service to which node port service will.Wrote is automatically created and here as you see if I list the services.The note port will have the cluster IP address and for each IP address.It will also have the ports open where the service is iaccessible at and also note that service spends all the worker nodes.So if you have 3 pod replicas on 3 different nodes.Basically, the service will be able to handle that request coming on any of the worker nodes and then forward it to one of those.</p>
<p>说话人 1 03:33:26<br>Pod replicas.Now that type of service exposure is not very efficient and also not secure because you are basically opening the ports to directly talk to the services on each worker node so the external clients basically.Have access to the worker nodes directly so if we gave all the services.</p>
<p>说话人 1 03:33:46<br>This node port service.Type then we would have a bunch of ports open on the worker nodes clients from outside can directly talk to.It’s not very efficient and secure way to handle that and is a better alternative.</p>
<p>说话人 1 03:34:00<br>There is a load balancer service type and the way it works with load balancer service type is that the service becomes accessible externally.Through a cloud providers load balancer functionality so each cloud provider has its own native load balancer implementation and that is created and used whenever we create a load balancer service type.</p>
<p>说话人 1 03:34:22<br>Google cloud platform, WS Azure Linode open stack and so on.All of them offer.This functionality so whenever we create a load balancer service.Note Portan cluster.I services are created.Automatically by Kubernetes to which the external load balancer of the cloud platform will route the traffic too.And this is an example of how we define load balancer service configuration so instead of node port type.We have a load balancer and the same way we have the port of the service, which belongs to the cluster IP.And we have the note port, which is the port that opens in the worker node.But it’s not directly accessible externally but only through the load balancer itself, so the entry point becomes a load balancer first.And it can then direct the traffic to node port on the worker node and the cluster.Ip the internal service, so that’s how the flow would work with a load balancer service.So, in other words, the load balancer service type is an extension of the node port type, which itself is an extension of the cluster.</p>
<p>说话人 1 03:35:34<br>Ip type and again if I create a load balancer service type.And least all the services you consider differences in the display as well, where for each service type.You see the IP addresses.You see the type and you see the ports that the service has opened.</p>
<p>说话人 1 03:35:54<br>An I should mention here that in a real community set of example.You would probably not use node port for external connection.You would maybe use it to test some service very quickly but not for production use cases.So for example, if you have a application that is iaccessible through browser.You will either configure Ingress for each such request.So you would have internal services, the cluster IP services that Ingress.Will Route 2 or you would have a load balancer that uses the cloud platforms.</p>
<p>说话人 1 03:36:24<br>Native load balancer implementation congratulations.You made it to the end.I hope you learned a lot and got some valuable knowledge from this course.If you want to learn about modern DevOps tools be sure to check out my tutorials on that topic and subscribe to my channel for more content.Also, if you want to stay connected.You can follow me on social media or join the private Facebook group.I would love to see you there, so thank you for watching and see you in the next video.</p>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/" rel="tag"><i class="fa fa-tag"></i> 云原生</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/04/16/%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E4%B8%8E%E5%88%86%E6%9E%90(2)/" rel="prev" title="数据管理与分析 - 事务">
      <i class="fa fa-chevron-left"></i> 数据管理与分析 - 事务
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFK8s"><span class="nav-number">1.</span> <span class="nav-text">什么是K8s</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#k8s%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">1.1.</span> <span class="nav-text">k8s解决的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">1.1.1.</span> <span class="nav-text">解决的问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K8s%E7%9A%84%E7%89%B9%E7%82%B9"><span class="nav-number">1.2.</span> <span class="nav-text">K8s的特点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Main-Components"><span class="nav-number">1.2.1.</span> <span class="nav-text">Main Components</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%87%E7%A8%BF%EF%BC%9A"><span class="nav-number">1.3.</span> <span class="nav-text">文稿：</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="尹 丹"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">尹 丹</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yoon286" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yoon286" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yoondara6@gmail.com" title="E-Mail → mailto:yoondara6@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/user123456" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;user123456" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

 <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">
  
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">

    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yoon</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
